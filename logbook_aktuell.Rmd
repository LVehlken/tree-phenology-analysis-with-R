---
output:
  html_document:
    css: style.css
  pdf_document: default
---
##### University of Bonn
<br/><br/>
<br/><br/>

###### Faculty of Agricultural, Nutritional and Engineering Sciences
<br/><br/>
<br/><br/>

##### Institute of Crop Science and Resource Conversation

<br/><br/>

#### Tree phenology analysis with R
<br/><br/>

##### Learning logbook Leon Vehlken

###### Winter term 2024/2025

###### Lecturer: Prof. Dr. Eike Lüdeling

###### 26.02.2025
<br/><br/>

###### Leon Vehlken

###### Aegidienberger Straße 23

###### 50939 Köln

###### lvehlken@smail.uni-koeln.de

###### Study programme: M.Sc. Geography 3. semester (University of Cologne)

###### Enrolment number: 7364968
<br/><br/>
<br/><br/>
<br/><br/>


This learning logbook records my learning as well as my coding work during the seminar "Tree phenology analysis with R". The seminar is based on the chillR package that was developed by Prof. Dr. Eike Lüdeling.

<br/><br/>
```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(magrittr)
library(kableExtra)
```
# Chapter 1 Introduction

## What was the chapter about?

The chapter gave an overview about the seminar, its objectives, the performance assessment (mainly in form of a learning logbook) and the house rules. 

## Chapter tasks

No tasks.

# Chapter 2 The tools we use

## What was the chapter about?

In this chapter we learned about the tools we will use for the phenology analysis. The analysis will be based on the coding language R. Therefore we will use the integrated development environment RStudio. We also learned about Git and GitHub which will use to store and work on our codes we will produce. In addition we learned about R Markdown, which is a R code format that is  helpful for creating reports or websites.

## Chapter tasks

No tasks.

# Chapter 3 Tree dormancy

## What was the chapter about?

The chapter was about the basics of tree dormancy. We learned what tree dormancy is, what the phases of dormancy are (endo dormancy and eco dormancy) and about the physiological processes that are behind it. Furthermore we learned about basic methods to calculate chilling and forcing periods.

## Chapter tasks

### 1. Put yourself in the place of a breeder who wants to calculate the temperature requirements of a newly released cultivar. Which method will you use to calculate the chilling and forcing periods? Please justify your answer.

To calculate the temperature requirements of a newly released cultivar I would use the statistical approach. During research, phenological data is collected for new cultivates. Therefore there should already be phenological data (flowering dates) available for the new cultivar.This data can be related to previous temperature records, using the Partial Less Square regression analysis to calculate chilling and forcing periods. Compared to other strategies, e.g. setting up an experiment with the cultivar to gather own empirical data for the temperature requirements, the statistical approach is faster and has lower expenses. 
     
### 2. Which are the advantages (2) of the BBCH scale compared with earlier scales?

Compared to earlier scales the growth stages can easily be recognized under field conditions with     the BBCH scale. Furthermore the stages are graded in the order of their appereance.
    
### 3. Classify the following phenological stages of sweet cherry according to the BBCH scale:

```{r, echo=FALSE, out.width="75%", warning=FALSE, message=FALSE}
knitr::include_graphics("D:/DatenVehlken/R/Projects/Chill_R/flowering_stages.png")
```

Figure 1: Three different flowering stages of sweet cherry.

On picture one (left) you can see the BBCH stage 57. The main stage is 50, reproductive development or influence emergence. The stage is 57 as the sepals are open but they don't form balloons yet.
On picture two (middle) you can see the BBCH stage 65. The main stage is 60, flowering, while the stage is 65: Full flowering as more than 40% percents of the flower is open.     
On the third picture the BBCH stage 85 can be seen. The cherries are in the ripening or maturity stage (80). The coloring is advanced but too light for a ripe fruit. Therefore it is in the stage 85.

# Chapter 4 Climate change and impact projection

## What was the chapter about?

## Chapter tasks

### 1. List the main drivers of climate change at the decade to century scale, and briefly explain the mechanism through which the currently most important driver affects our climate.

Greenhouse gas emission is the currently most important driver for climate change. The short wave radiation emitted by the sun passes through the atmosphere. The energy is absorbed by the earth(surface) and emitted as long wave radiation. The long wave radiation, in contrast to the short wave, gets absorbed by the greenhouse gases that are accumulated in the atmosphere, thereby trapping the head. Therefore, the more greenhouse gasses are emitted, the more heat is trapped insite the climate system.
Other climate change drivers are land use changes, volcanic activities, aerosols, the variability in solar radiation and changes in the ocean circulation.

### 2. Explain briefly what is special about temperature dynamics of recent decades, and why we have good reasons to be concerned.

The temperature dynamics of recent decades are special as the temperatures are rising rapidly compared to the past (~1.1°C since 1900. We should be concerned as we will experience more extreme weathers. E.g. the atmosphere can absorb more water due to the high energy level. This can on the hand lead to more and longer droughts and on the other hand to extreme rain events. 

### 3. What does the abbreviation ‘RCP’ stand for, how are RCPs defined, and what is their role in projecting future climates?

RCP stands for Representative Concentration Pathways. The four RCPs(2.6,4.5,6 & 8) are defined by the expected change in radiative forcing (W/m²) from 1750 to 2100. E.g. RCP2.6 stands for a change of 2.6W/m². They stand for different climate change scenarios that seemed to be possibly for the future and are used to model future climate. 

### 4. Briefly describe the 4 climate impact projection methods described in the fourth video.

The climate projection methods described in the video are statistical models, species distribution models, process-based models and the climate analogue analysis. Statistical models use observed climate data together with statistical relationshipts to predict future climate. Species distribution use environmental background data (e.g. remote sensing data) at known locations of a species to predict their possible distribution. Together with future climate model data they can be used to predict possible species habitats in the future. Process based models try to represent all systems processes with equation. To model these processes they use state of the art scientific knowledge. Climate analogue analysis tries to find locations that currently have the climatic conditions you expect for the area of your interest in the future. You can look how the location with your feature climate adapted to it and get inspiration how to addept to the climate change at your location.


# Chapter 5 Winter chill projections

## What was the chapter about?

Chapter 5 was about winter chill projections and gave an overview of how winter chill can be modeled and showed the problems and challenges that had to be overcome to create state-of-the-art chill models. 

## Chapter tasks

### 1. Sketch out three data access and processing challenges that had to be overcome in order to produce chill projections with state-of-the-art methodology.

One challenge that had to be overcome and is still problematic is the model choice. For the same  location different chill models generate very different results and therefore it is hard to distinguish which one produces meaningful results. 
Another challenge that had to be overcome was to find feature climate data that is suitable for chill modelling. While there is a lot of feature climate data the data is most of the time stored in grids for the whole world which leads to huge data amounts that can't really be handled. By the time weather generators for specific region were developed, allowing to get data easier.
Most climate data was in the daily format, while for chill modeling hourly temperature is needed. The used methods to derive hourly temperature data from daily temperature data were not optimal and had to be overcome.

### 2. Outline, in your understanding, the basic steps that are necessary to make such projections.

The first step is to get climate projection data for your area of interest (AOI). Therefore you can model future climate data on your own using weather generators, based on reliable weather station data from your AOI. An alternative is to use climate model data provided by others like the AFRICLIM and extract the data for your AOI.                               The second step is to choose one (wisely) or more Chill models to model the impacts of climate change on winter chill
In a third step, for the results you generated you calculate metrics like 'Safe Winter Chill'(10^th^ percentile of the chill distribution) and condense them to enable the data for analysis. 

# Chapter 6 Manual chill analysis

## What was the chapter about?

## Chapter tasks

### 1. Write a basic function that calculates warm hours (>25°C).

To write the function we use the function() tool of R, which provides the base mechanisms for defining new functions. The function will be called WH() and can be run by an argument that contains a data set. The data set needs a column "Temp" that has hourly temperature data. The function creates the new columm "Warm_Hour" in the data set in checks for every row if the value of Temp is above 25. If it is above 25 it creates a new vector TRUE or FALSE
```{r, warning=FALSE, message=FALSE}
# defining function WH to identify which hours from a data set are above 25°C. 

WH <- function(data_set)
{
  data_set[,"Warm_Hour"] <- 
    data_set$Temp > 25
  return(data_set)
}

```

### 2. Apply this function to the Winters_hours_gaps dataset

Our new created function we can just apply to the Winters_hours_gaps dataset by using the data set name as an argument. The Winters_hours_gaps data set is included in the chillR package, therefore I first load it using the library()-function. In Addition I load the tidyverse-package which contains a lot of useful packages I need later, e.g. ggplot2 for plotting results.
```{r, message=FALSE, warning=FALSE}
library(chillR)
library(tidyverse)
```

Applying the function to the "Winter_hours_gaps" data set. By using the argument [1:8,] behind the function we restrict the function to the rows 1 to 8, as I'm not interested for the whole output.
```{r, warning=FALSE, message=FALSE}
W_H <- WH(Winters_hours_gaps)
```
Table 1: Output table of the WH()-function. Winters_hours_gaps data set with hours classified as > 25°C or < 25°C.
```{r,echo=FALSE}

kable(head(W_H, 7)) %>% 
  kable_styling("striped", position = "left", font_size = 10)
```
 
### 3. Extend this function, so that it can take start and end dates as inputs and sums up warm hours between these dates

To extent the function to sum up warm hours between start and end dates I give my warm hours function two new arguments, the start and the end date. For that I use the YEARMODA format. I generate a YEARMODA value for every row of my data set, identify the warm hours by using my warm hours function and sum up the warm hours for the rows where the YEARMODA value is between the values of my start and end date arguments.
```{r, results='hide', warning=FALSE, message=FALSE}
# generating a function that calculate the sum for all warm hours for a given data set between two specified dates. 

WH_sum <- function(data_set,
                   Start_YEARMODAHO,
                   End_YEARMODAHO)
  
# as my data only contains columns for year, month and day, the first step is to compute the YEARMODA value for every row.
{
  Start_Year <- trunc(Start_YEARMODAHO / 10000) # "trunc" removes all decimals
  Start_Month <-
    trunc((Start_YEARMODAHO - Start_Year*10000) / 100)
  Start_Day <- 
    Start_YEARMODAHO - Start_Year * 10000 - Start_Month * 100
  Start_Hour <- 12 
  End_Year <- trunc(End_YEARMODAHO / 10000)
  End_Month <- trunc((End_YEARMODAHO - End_Year * 10000) / 100)
  End_Day <- End_YEARMODAHO - End_Year * 10000 - End_Month * 100
  End_Hour <- 12 

  Start_YEARMODAHO <- which(data_set$Year == Start_Year &
                        data_set$Month == Start_Month &
                        data_set$Day == Start_Day &
                        data_set$Hour == Start_Hour)
  End_YEARMODAHO <- which(data_set$Year == End_Year &
                    data_set$Month == End_Month &
                    data_set$Day == End_Day &
                    data_set$Hour == End_Hour)

  Warm_hours <- WH(data_set)
  # to calculate the sum of the warm hours I now sum the values of the Warm_hours column for the rows where the YEARMODA value is between the values specified by Start_YEARMODAHO & End_YEARMODAHO.
  return(sum(Warm_hours$Warm_Hour[Start_YEARMODAHO:End_YEARMODAHO]))

}

# run the function to calculate the amount of warm hours between the 01.05.2008 and the 11.07.2008.
WH_sum(Winters_hours_gaps, 20080501, 20080711)
```

Between the 01.05.2008 and the 11.07.2008 we had 479 warm hours. 

# Chapter 7 Chill models

The chapter was about how to caluclate Chilling Hours. For that a basic function (Chilling_Hours) of the chillR package was used Furthermore we learned how to use other models like the Dynamic Model, which is the state of the art model, to calculate chill and even how to generate our own model.

## What was the chapter about?

## Chapter tasks

### 1. Run the chilling() function on the Winters_hours_gap dataset

The chilling() tool of the chillR package can be used to calculate the four horticultural chill metrics Chilling Hours, Chilling Units (based on Utah Model), Chill Portions (based on Dynamic Model) and Growing Degree Hours. As input it needs a data set with hourly temperatures with Julian days. The start and end days of the calculation can be specified. For the Winters_hours_gaps data set we compute them for the Julian days 90 to 100.
    
```{r, warning=FALSE, message=FALSE}
output <- chilling(make_JDay(Winters_hours_gaps),
                   Start_JDay = 90,
                   End_JDay = 100)
```
Table 2: Chill metrics for the Winters_hours_gaps data set based on Chilling Hours-, Utah-, Dynamic- and Growing Degree Hours models.
```{r, echo=FALSE}
kable(head(output, 7)) %>% 
  kable_styling("striped", position = "left", font_size = 10)
```

### 2. Create your own temperature-weighting chill model using the step_model() function & 3. Run this model on the Winters_hours_gaps dataset using the tempResponse() function

The chilling()-function is limited to the four metrics described above. With the tempResponse()-function of chillR you can calculate metrics for models that you specify in a list, allowing to use your own model. As example we create our own climate model(custom()) using a step model. A step model applies to a given vector (x) the step function that is specified in a data frame (df).
```{r, warning=FALSE, message=FALSE}
custom <- function(x) step_model(x, df)

# the data frame used consists of three columns. The lower column specifies the lower bound of the temperature interval, while the upper column specifies the upper bound. The weight column specifies the value that is assigned to the temperature falling into the specific temperature interval.

df<-data.frame(
  lower= c(-1000, 2,4, 6, 8, 10,    12),
  upper= c(    2, 4, 6, 8, 10, 12, 1000),
  weight=c(    0, 1, 2, 3, 2, 1,    0))

# the model we can then use with the tempResponse function.
output <- tempResponse(make_JDay(Winters_hours_gaps),
                       Start_JDay = 90,
                       End_JDay = 100,
                       models = list(custom = custom))

```
Table 3: Chill Metric for the Winters_hours_gaps data set based on the custom()-model.
```{r, echo=FALSE}
kable(head(output, 7)) %>% 
  kable_styling("striped", position = "left", font_size = 10)
```

# Chapter 8 Making hourly temperatures

## What was the chapter about?

To model chill hourly temperature is needed. As temperature data often is only recorded by daily minimum and maximum we learned how to generate hourly temperature data from daily data sets, based on idealized temperature curves. As idealized curves don't work for all locations we also learned how to generate hourly temperature data from empirical daily temperature curves.

## Chapter tasks

### 1. Choose a location of interest, find out its latitude and produce plots of daily sunrise, sunset and daylength.

The location of interest is the Dehesa San Francisco (37°52' N,6°14' W; 445 m) in the south-west of the Iberian Peninsula.
```{r, out.width="75%", warning=FALSE, message=FALSE, echo=FALSE}
knitr::include_graphics("D:/DatenVehlken/R/Projects/Chill_R/Dehesa.png")
```

Figure 2: Location of interest: the Dehesa San Francisco.

The area covers 516 ha and is characterized by north- and south-facing slopes.
According to the climate classification of Köppen-Geiger, the climate falls into the class Csa with winter rainfall and dry, hot summers (Kottek et al., 2006).
To plot the daily sunrise, sunset and day length we first compute them for the days 1 to 365 of a year at the latitude of the Dehesa San Francisco, using the daylength() tool from the ChillR package and save them to the object "Days". 
```{r, warning=FALSE, message=FALSE}
Days <- daylength(latitude = 37.881017 , JDay = 1:365)

# in a second step we create a data.frame() with the columns Sunrise, Sunset, Daylength and JDay from the Days object, as we want to plot the data with ggplot.
Days_df <-
  data.frame(
    JDay = 1:365,
    Sunrise = Days$Sunrise,
    Sunset = Days$Sunset,
    Daylength = Days$Daylength
  )

# using the pivot_longer function. 
Days_df <- pivot_longer(Days_df,cols=c(Sunrise:Daylength))

ggplot(Days_df, aes(JDay, value)) +
  geom_line(lwd = 1.5) +
  facet_grid(cols = vars(name)) +
  ylab("Time of Day / Daylength (Hours)") +
  theme_bw(base_size = 20)
```

Figure 3: Daily sunrise, sunset and day length at the Dehesa San Francisco.

### 2. Produce an hourly dataset, based on idealized daily curves, for the KA_weather dataset (included in chillR). 

Instead of the KA_weather data set I used a data set with temperature records for the Dehesa San Francisco for this task (http://www.juntadeandalucia.es/agriculturaypesca/fitEmaWeb/faces/pages/infoEstacion.xhtml?id=102).
To compute chill hours we are dependent on hourly temperature data. Nevertheless, for many locations only daily minimum and maximum temperatures are available. Different statistical approaches enable us to calculate hourly temperature data from daily data sets. For locations with a flat relief idealized temperature curves can be computed, based on the times of sunrise & sunset and the day length. 
```{r, warning=FALSE, message=FALSE}

# import the climate data using the read.csv() function.

Dehesa_weather <- read.csv("D:/DatenVehlken/Sciebo/Uni/Geographie/Bachelor/Bachelorarbeit/Daten/KlimadatenDehesa3.csv")
```

To generate hourly data from the daily data data set we use the stack_hourly_temps() function that calculates the hourly temperatures using the daily dynamics (Sunrise, Sunset, Day length) and idealized daily curves.
```{r, warning=FALSE, message=FALSE}
Dehesa_hourly <- stack_hourly_temps(Dehesa_weather,
                                    latitude=37.881017,
                                    keep_sunrise_sunset = TRUE)
```
Table 4: Hourly temperature data for the Dehesa San Francisco.
```{r,echo=FALSE}
kable(head(Dehesa_weather, 7)) %>% 
  kable_styling("striped", position = "left", font_size = 10)
```

The produced data.frame has single columns for year, month, day and hour. As I want to plot the hourly temperatures by date I have to aggregate the date from these columns. Therefore the ISOdate() function is used. 
```{r, warning=FALSE, message=FALSE}
Dehesa_hourly$hourtemps[, "DATE"] <-
  ISOdate(
    Dehesa_hourly$hourtemps$Year,
    Dehesa_hourly$hourtemps$Month,
    Dehesa_hourly$hourtemps$Day,
    Dehesa_hourly$hourtemps$Hour
  ) 
```

### 3. Produce empirical temperature curve parameters for the Winters_hours_gaps dataset, and use them to predict hourly values from daily temperatures.

As mentioned above the idealized temperature curve approach only works for areas with flat relief. For areas with high relief energy (e.g. the area of the Dehesa San Francisco) they don't work as they are shaded by rocks ,etc. for parts of the day. For those regions empirical temperature curves have to be generated, which rely on observed hourly temperature data.
With the Empirical_daily_temperature_curve() function you can derive an empirical daily temperature curve from the data set with observed hourly temperatures. The function produces a table with typical temperatures coefficients for every hour for every month. These coefficients can again be used together with daily minimum and maximum temperature to generate hourly temperate data that is based on the empirical relationship between them, using the Empirical_hourly_temperatures() function.

```{r, warning=FALSE, message=FALSE}

# derive the empirical daily temperature curve.

coeffs <- Empirical_daily_temperature_curve(Winters_hours_gaps)

# as the winters_hours_gaps dataset only provides hourly data we first calculate Tmin and Tmax using the make_all_day_table() function. 


Winters_daily <-
  make_all_day_table(Winters_hours_gaps, input_timestep = "hour")

# use the coefficients and the daily Tmin & Tmax as input to compute the empirical hourly temperatures.

Winters_hours <- Empirical_hourly_temperatures(Winters_daily, coeffs)

Winters_hours[, "DATE1"] <-
  ISOdate(Winters_hours$Year,
          Winters_hours$Month,
          Winters_hours$Day,
          Winters_hours$Hour)

Winter_hours_plot <- Winters_hours[100:200,]

# plot the results.

ggplot(data = Winter_hours_plot, aes(DATE1, Temp, colour = "RED")) +
  geom_line(lwd = 1.3) + ylab("Temperature (°C)") + xlab("Date")

```

Figure 4: Empirical daily temperature curves between march seven and eleven for the Winters_hours_gaps data set. 

# Chapter 9 Some useful tools in R

## What was the chapter about?

Chapter 9 was about useful tools in R that make coding easier. We were introduced to tidyverse, which is a compangion of different R packages that are often used by scientist. Furthermore we learned about loops, which allow to iterate code many times automatically instead of having to do it manually.

## Chapter tasks

### 1. Based on the Winters_hours_gaps dataset, use magrittr pipes and functions of the tidyverse to accomplish the following:
        a. Convert the dataset into a tibble
        b. Select only the top 10 rows of the dataset
        c. Convert the tibble to a long format, with separate rows for Temp_gaps and Temp
        d. Use ggplot2 to plot Temp_gaps and Temp as facets (point or line plot)
        e. Convert the dataset back to the wide format
        f. Select only the following columns: Year, Month, Day and Temp
        g. Sort the dataset by the Temp column, in descending order
        
### a. 
first we convert our data.frame into a tibble using the as_tibble function. A tibble is an improved version of the data.frame.
```{r, warning=FALSE, message=FALSE}

require(chillR)


WHG_tibble <- as_tibble(Winters_hours_gaps)
```
Table 7: Winters_hours_gaps data set in tibble format
```{r, echo=FALSE}
kable(head(WHG_tibble, 7)) %>% 
  kable_styling("striped", position = "left", font_size = 10)
```

### b.
With the same function we can convert the first ten rows of the data set to a new tibble by specifying the rows in the bracket [1:10, ].

```{r, warning=FALSE, message=FALSE}
WHG_ten_rows <- as_tibble(WHG_tibble[1:10, ])
```
Table 8: First ten rows of the WHG_tibble
```{r, echo=FALSE}
kable(head(WHG_ten_rows, 7)) %>% 
  kable_styling("striped", position = "left", font_size = 10)
```

### c.
Our data set has separate columns for temperature (Temp) and temperature gaps (Temp_gaps). Using the pivot_longer together with magrittr pipes (%>%) we can aggregate the two columns into distinct rows.
```{r, warning=FALSE, message=FALSE}
WHGlong <- WHG_tibble %>% pivot_longer(cols = Temp_gaps:Temp)

```
Table 8: WHG tibble in long format.
```{r, echo=FALSE}
kable(head(WHG_ten_rows, 7)) %>% 
  kable_styling("striped", position = "left", font_size = 10)
```
### d.
Ggplot is a plotting tool for R allowing to create highly customized plots. Here we create scatter plot for the converted tibble.
```{r, warning=FALSE, message=FALSE}
ggplot(data = WHGlong, mapping = aes(x = Hour, y = value, color = name)) +
  geom_point() + # use geom_point() for scatterplot instead
  labs(title = "Temperature and Temperature Gaps Over Time",
       x = "Hour",
       y = "Value") +
  theme_minimal()
```

Figure 5: Hourly temperature (red) and temperature gaps(blue) over time for the Winters_hours_gaps data set.

### e.
With the pivot_wider() function we can convert our tibble in the long format back to the wide format it was originally stored in.
```{r, warning=FALSE, message=FALSE}
WHGwide <- WHGlong %>% pivot_wider(names_from = name,
                                   values_from = value) 
WHGwide <- WHGlong %>% pivot_wider() 
```
Table 9: WHG tibble in wide format.
```{r, echo=FALSE}
kable(head(WHGwide, 7)) %>% 
  kable_styling("striped", position = "left", font_size = 10)
```

### f.
The select()-function can be used to select specific columns of interest from a data set. 
```{r, warning=FALSE, message=FALSE}
# select
WHG_selected <- WHGwide %>% select(c(Year, Month, Day, Temp))
```
Table 10: Columns year, month, day and temperature selected from the WHG tibble.
```{r}
kable(head(WHG_selected, 7)) %>% 
  kable_styling("striped", position = "left", font_size = 10)
```

### g. 
With the arrange() function we can rearrange our data. Here we sort it by temperature by descending order.
```{r, warning=FALSE, message=FALSE}
WHG_arranged <- WHGwide %>% arrange(desc(Temp))
```
Table 11: WHG tibble arranged by descaning temperature.
```{r,echo=FALSE}
kable(head(WHG_arranged, 7)) %>% 
  kable_styling("striped", position = "left", font_size = 10)
```

### 2. For the Winter_hours_gaps dataset, write a for loop to convert all temperatures (Temp column) to degrees Fahrenheit

Loops can be used to run the same action again for a specified data range instead of doing the same tasks manually, saving a lot of time. In R loops can be performed using the for(variable in vector){expression}-function. This function performs for every variable in a vector the expression specified in {}.
```{r, warning=FALSE, message=FALSE}
# the following loop converts for every variable(i) in the rows 1 to the last row of the table WHGwide  the temperature in °C to the temperature in Fahrenheit.

for (i in 1:nrow(WHGwide)) {
  WHGwide$Temp[i] <- WHGwide$Temp[i] * (9/5) + 32
}

```
Table 12: Temperature of the WHG tibble in ° Fahrenheit
```{r,echo=FALSE}
# print the updated data set.
kable(head(WHGwide,7)) %>%
  kable_styling("striped", 
                position = "left",
                font_size = 10)
```

# 3. Execute the same operation with a function from the apply family

The same conversion can be done by using the sapply(vector, function) function. The function applies to every vector a specified function. 
```{r, warning=FALSE, message=FALSE}
WHGwide$Temp <- sapply(WHGwide$Temp, function(temp) temp * (9/5) + 32)
```
Table 13: Temperature of the WHG tibble in ° Fahrenheit
```{r, echo=FALSE}
# print the updated data set.
kable(head(WHGwide,7)) %>%
  kable_styling("striped", 
                position = "left",
                font_size = 10)
```


# 4. Now use the tidyverse function mutate to achieve the same outcome

The same can be done by using the mutate()-function, following the same principle.
```{r, warning=FALSE, message=FALSE}

WHGwide <- WHGwide %>%
  mutate(Temp = Temp * (9/5) + 32)
```
Table 14: Temperature of the WHG tibble in ° Fahrenheit
```{r, echo=FALSE}
# print the updated data set.
kable(head(WHGwide,7)) %>%
  kable_styling("striped", 
                position = "left",
                font_size = 10)
```

# Chapter 10 Getting temperature data

## What was the chapter about?

The chapter was about how to get temperature data, as it is the base for phenology and chill modeling. We were introduced to different databases and learned how to download it for the use in chillR.

## Chapter tasks

### 1. Choose a location of interest and find the 25 closest weather stations using the handle_gsod function

For chill modelling we are dependent on temperature data for our area of interest. An straight forward way to access this data is to download it from the Global Summary of the Day (GSOD) data base, using the handle_gsod() function provided by the chillR package. For that it has four action types: "list_stations", "download_weather", "delete" or clean the data frame. 
My location of interest is the Dehesa San Francisco described in chapter 8. To find possible data sets for my location I first generate a list of stations (action="list_stations") near my location that provide data for a time_interval of my choice. Here I use 30 years as time interval as it is the minimum requirement to calculate a climate normal.
```{r, warning=FALSE, message=FALSE}
station_list <- handle_gsod(action="list_stations",
                          location=c(-6.2333,37.8667),
                          time_interval=c(1990,2020))
```
Table 15: List of climate stations near the Dehesa San Francisco that are contained in the GSOD data base
```{r,echo=FALSE}
kable(head(station_list, 7)) %>% 
  kable_styling("striped", position = "left", font_size = 10)
```

### 2. Download weather data for the most promsing station on the list.

In a second step I download the weather data (action="download_weather") for the most promising station on the list. The most promising station is the station nearest to my location that has data available for the time span I'm interested in. In this case it is the first station on the list: "SEVILLA" (chillR_code = 08391099999) in a distance of 58.27km. 

```{r, warning=FALSE, message=FALSE,eval=FALSE}
weather<-handle_gsod(action="download_weather",
                     location=station_list$chillR_code[1],
                     time_interval=c(1990,2020))
```

### 3. Convert the data into chillR format

In the next step I drop redundant data from the downloaded data by using the clean action of the handle_gsod-function, and transform it into a data set that is suitable for chillR.

```{r, warning=FALSE, message=FALSE, eval=FALSE}
cleaned_weather<-handle_gsod(weather)

# as the function produced a list containing a data.frame instead of a data.frame I have to convert it. The data.frame is at the first position of the list. Therefore I assign the first object of the list to a new object, using [[]]. 

df_cleaned_weather <- cleaned_weather[[1]]  # 

# as the first column "date" is not suitable for chillR I create a new table with the columns two to eight.

cleaned_weather <- df_cleaned_weather[, 2:8]
```
Table 16: Weather data in chillR format for the climate station Sevilla.
```{r,echo=FALSE}
cleaned_weather <- read_tab("data/Dehesa_chillR_weather.csv")
```
```{r,echo=FALSE}
kable(head(cleaned_weather, 7)) %>% 
  kable_styling("striped", position = "left", font_size = 10)
```
```{r, warning=FALSE, message=FALSE, echo=FALSE, eval=FALSE}
write.csv(station_list,"data/station_list.csv",row.names=FALSE)
write.csv(weather[[1]],"data/Dehesa_raw_weather.csv",row.names=FALSE)
write.csv(cleaned_weather,"data/Dehesa_chillR_weather.csv",row.names=FALSE)
```

# Chapter 11 Filling gaps in temperature records

## What was the chapter about?

In chapter 11 we learned how to fill temperature gaps that is contained in data due to e.g. hardware errors. Short gaps can be interpolated while longer gaps should be filled with data from weather stations nearby. 

## Chapter tasks

### 1. Use chillR functions to find out how many gaps you have in this dataset (even if you have none, please still follow all further steps)
    
Most likely the downloaded data has some gaps that e.g. can occur due to measuring or hardware errors. Therefore it is important to check if there are gaps. 
```{r, warning=FALSE, message=FALSE}
library(naniar)

# the data gaps are often not marked, instead the specific rows are just missing. The make_all_day_table() tool adds the missing records to the dataset and allows to screen for the gaps. 
weather <- cleaned_weather %>% make_all_day_table()

# after creating the missing records we select the variables we want to screen and check with vis_miss() for missing data.
weather %>%
  dplyr::select(Tmin, Tmax, Prec) %>%
  naniar::vis_miss()
```

Figure 6: Gaps in the temperature data for the station Sevilla between 1990 and 2020.

In this case 0.5% of the data is missing. This is data for about 57 days.

### 2. Create a list of the 25 closest weather stations using the handle_gsod function & 3.Identify suitable weather stations for patching gaps

While short gaps can be interpolated, it is not recommended for longer gaps as it leads to bias. Instead, to fill these gaps we can use weather data from other weather stations that are near the location. Therefore we again can use the handle_gsod()-tool. First we scan the gsod database for stations and create a new list. 
```{r, warning=FALSE, message=FALSE}
station_list <- handle_gsod(action="list_stations",
                            location=c(-5.893,37.418),
                            time_interval=c(1990,2020))
```
Table 17: Weather stations near the Sevilla station, contained in the GSOD data base.
```{r,echo=FALSE}
kable(head(station_list, 7)) %>% 
  kable_styling("striped", position = "left", font_size = 10)
```

The table above shows the other stations in the area of our location. Now I want to identify weather stations that are suitable to fill the gaps. Therefore the station should be close to my originally used station and should also cover the time span I'm using data for. In this case the station "MORON AB" (chillR code = 08397099999) seems to be a good match. The station is only ~36.5km away from my station and covers the whole period. Therefore I chose this station to patch my data. Nevertheless, it turned out that the station couldn't fill my gap completly and that it was the same case for many other stations. Therefore I devided to download data from seven different stations. 

### 4. Download weather data for promising stations, convert them to chillR format and compile them in a list

To patch my weather data with the data of the station(s) I chose for patching I first have to download it. For downloading the data I again use the "download_weather" action of the handle_gsod function.
```{r, warning=FALSE, message=FALSE}
patch_weather<-
      handle_gsod(action = "download_weather",
                  location = as.character(station_list$chillR_code[c(4,6,8,9,12,13,14)]),
                  time_interval = c(1973,2019)) %>%
  handle_gsod() # to clean the data and convert them to chillR format I run the handle_gsod() cleaning function for every element of the list produced with the download action, by calling it with the magrittr pipes.
```

### 5. Use the patch_daily_temperatures function to fill gaps

This data I can now use in the patch_daily_temperatures()-tool. The tool fills the gaps in the weather record data set with data from the list with data off other weather stations.
```{r, warning=FALSE, message=FALSE}
patched <- patch_daily_temperatures(weather = weather,
                                    patch_weather = patch_weather)

# after patching the weather data I can again scan for gaps with the workflow already shown.
df_patched_weather <- patched[[1]]  # 



patched_weather <- df_patched_weather[, 3:9]


# the results show that there are still small gaps in the data.
patched_weather %>%
  dplyr::select(Tmin, Tmax, Prec) %>%
  naniar::vis_miss()
```

Figure 7: Gaps in the temperature data for the station Sevilla between 1990 and 2020 after patching it with data from stations nearby.


As none of the weather stations contained in the list provides data to close all gaps I interpolate the remaining gaps using the fix_weather()-tool.
```{r, warning=FALSE, message=FALSE}
fixed_all_days <- fix_weather(patched_weather,
                              start_year = 0,
  end_year = 3000,
  start_date = 1,
  end_date = 366,
  columns = c("Tmin", "Tmax", "Prec"),
  end_at_present = TRUE
)

# the check shows that all gaps are closed now.
fixed_all_days <- fixed_all_days[[1]]
fixed_all_days %>%
  dplyr::select(Tmin, Tmax, Prec) %>%
  naniar::vis_miss()

```

Figure 8: Gaps in the temperature data for the station Sevilla between 1990 and 2020 after interpolating the last gaps.

Now all weather gaps are closed and the data is ready for further analysis. In the end I save my data again to be able to use it later.

```{r, warning=FALSE, message=FALSE,echo=FALSE,eval=FALSE}
write.csv(fixed_all_days, "D:/DatenVehlken/Sciebo/Uni/Geographie/Master/Nebenfach/tree_phenology/climate_data/dehesa_patched.csv")
```

# Chapter 12 Generating temperature scenarios.

## What was the chapter about?? 

In chapter 12 we learned about the importance of temperature scenarios. Grower need information about the agroclimatic characteristics of their sites to make the right planting decisions. With the knowledge they can gain from temperature scenarios that represent the conditions at their sites in the future they can conduct risk assessments for their planned cultivars. These temperature scenarios can be generated with weather generators that model the future climate at a given location by using observed temperature data. 

## Chapter tasks

### 1. For the location you chose for your earlier analyses, use chillR’s weather generator to produce 100 years of synthetic temperature data.
  
```{r, warning=FALSE, message=FALSE}
# first I load the patched weather data for my location.

data <- read.csv("D:/DatenVehlken/Sciebo/Uni/Geographie/Master/Nebenfach/tree_phenology/climate_data/dehesa_patched.csv")
```

To generate synthetic weather I use the temperature_generation(years, sim_years)-function, which calculates synthetic weather data for a given time span based on observed weather data. Here I generate synthetic weather data for the years 2001 to 2100 based on the weather records of the years 1998 to 2009.
```{r, warning=FALSE, message=FALSE}
Temp <- data %>%
  temperature_generation(years = c(1998,2009),
                         sim_years = c(2001,2100))
```
```{r,echo=FALSE,eval=FALSE}
write.csv(Temp,"D:/DatenVehlken/Sciebo/Uni/Geographie/Master/Nebenfach/tree_phenology/climate_data/dehesa_synth.csv")
```

### 2. Calculate winter chill (in Chill Portions) for your synthetic weather, and illustrate your results as histograms and cumulative distributions

To calculate winter chill for my synthetic weather data I need to hourly temperature data. Therefore I again use the stack_hourly_temp()-function. Nevertheless, I'm also interested in the winter chill of my observed data and therefore I want to compare them.
To compare the observed and the synthetic data I create a new data frame containing both data sets. Therefore I create the new column "Data_source" in both data set, classify it as "observed" or "simulated" and filter the data set for the wanted variables or time spans. After that I merge the data sets and create a new column "Date" with a fixed year. This allows to plot the data easier for comparison as it is less influenced by variability in the month between the years.
```{r, warning=FALSE, message=FALSE}

# filtering the data set with observed temperature data for years of interest.
Temperatures <- data %>% filter(Year %in% 1998:2009) %>%
  cbind(Data_source = "observed")

# select columns of interest from data set with synthetic temperature data.
new_data <- Temp[[1]] %>%
  select(Year, Month, Day, Tmin, Tmax) %>%
  cbind(Data_source = "simulated")

# combine the selected data.

Temperatures <- bind_rows(Temperatures, new_data)

# creating column with the fixed year.

Temperatures <- Temperatures %>% 
  mutate(Date = as.Date(ISOdate(2000, Month, Day))) 

```
```{r, echo=FALSE}
kable(head(Temperatures,7)) %>%
  kable_styling("striped", 
                position = "left",
                font_size = 10)
```


From the new data set I now calculate hourly data as well as the chilling hours with the already known work flows and save the results into a new data frame. 
```{r, warning=FALSE, message=FALSE}
chill_observed <- Temperatures %>%
  filter(Data_source == "observed") %>%
  stack_hourly_temps(latitude = 37.8667) %>%
  chilling(Start_JDay = 305,
           End_JDay = 59)
  
chill_simulated <- Temperatures %>%
  filter(Data_source == "simulated") %>%
  stack_hourly_temps(latitude = 37.8667) %>%
  chilling(Start_JDay = 305,
           End_JDay = 59)
  
chill_comparison <-
  cbind(chill_observed,
        Data_source = "observed") %>%
  rbind(cbind(chill_simulated,
              Data_source = "simulated"))

# to avoid bias through missing observations I filter the data for complete chill calculation periods. 
chill_comparison_full_seasons <- 
  chill_comparison %>%
  filter(Perc_complete == 100)

chill_comparison_full_seasons$Data_source <- factor(
  chill_comparison_full_seasons$Data_source,
  levels = c("simulated", "observed")
)
```

Now I can plot the results for the winter chill for the synthetic weather data.
```{r, warning=FALSE, message=FALSE}
ggplot(chill_comparison_full_seasons,
       aes(x = Chill_portions)) + 
  geom_histogram(binwidth = 1, position = "identity",
                 aes(fill = factor(Data_source))) +
  scale_fill_brewer(palette="Set2") +
  theme_bw(base_size = 20) +
  labs(fill = "Data source") +
  xlab("Chill accumulation (Chill Portions)") +
  ylab("Frequency")
```

Figure 9: Histogram of the Chill Portions frequency (green for simulated data, orange for observed) at the Dehesa San Francisco.

```{r, warning=FALSE, message=FALSE}
chill_simulations <-
  chill_comparison_full_seasons %>%
  filter(Data_source == "simulated")
  
ggplot(chill_simulations,
       aes(x = Chill_portions)) +
  stat_ecdf(geom = "step",
            lwd = 1.5,
            col = "blue") +
  ylab("Cumulative probability") +
  xlab("Chill accumulation (in Chill Portions)") +
  theme_bw(base_size = 20)
```

Figure 10: Cumulative probability of chill accumulation in Chill Portions at the Dehesa San Francisco.

### 3. Produce similar plots for the number of freezing hours (<0°C) in April (or October, if your site is in the Southern Hemisphere) for your location of interest.

With nearly the same procedure we can also plot the number of freezing hours for a specific month. Therefore we use the tempResponse()-tool instead of the chilling()-tool. With tempResponse() you can calculate temperature metrics specified by a model. To calculate freezing hours we first define a freezing model that we than use in the tempResponse-function.
```{r, warning=FALSE, message=FALSE}

# the freezing hours model we use here is a step model working similar like the one described in chapter 7.

freezing_hours <- function(x) step_model(x,df)

df <- data.frame(
  lower= c(-1000, 0),
  upper= c(    0, 1000),
  weight=c(    1, 0))

chill_observed_april <- Temperatures %>%
  filter(Data_source == "observed") %>%
  stack_hourly_temps(latitude = 37.418) %>%
  tempResponse(Start_JDay = 91,
           End_JDay = 120,
           models = list(Frost = freezing_hours))

chill_simulated_april <- Temperatures %>%
  filter(Data_source == "simulated") %>%
  stack_hourly_temps(latitude = 37.418) %>%
  tempResponse(Start_JDay = 91,
           End_JDay = 120,
           models = list(Frost = freezing_hours))

chill_comparison_april <-
  cbind(chill_observed_april,
        Data_source = "observed") %>%
  rbind(cbind(chill_simulated_april,
              Data_source = "simulated"))
```

The data I can now use to plot my freezing hours frequency as well as the freezing accumulation.
```{r, warning=FALSE, message=FALSE}

ggplot(chill_comparison_april,
       aes(x = Frost)) + 
  geom_histogram(binwidth = 5,
                 aes(fill = factor(Data_source))) +
  theme_bw(base_size = 10) +
  labs(fill = "Data source") +
  xlab("Frost incidence during winter (hours)") +
  ylab("Frequency")
```

Figure 11: Frequency of frost hours (turquoise for simulated data, orange for observed) at the Dehesa San Francisco.

When plotting the histogram for the frost hours I get a weird looking plot. This is due to the fact that there are zero frost hours at the location and the ggplot algorithm produces a "good" range for plotting automatically. A comparable "problem" occurs for the freezing accumulation plot.
```{r, warning=FALSE, message=FALSE}
chill_simulations_april <-
  chill_comparison_april %>%
  filter(Data_source == "simulated")
  
ggplot(chill_simulations_april,
       aes(x = Frost)) +
  stat_ecdf(geom = "step",
            lwd = 1.5,
            col = "blue") +
  ylab("Cumulative probability") +
  xlab("freezing accumulation (in freezing Portions)") +
  theme_bw(base_size = 20)
```

Figure 12: Cumulative probability of freezing accumulation at the Dehesa San Francisco.

# Chapter 13 Saving and loading data (and hiding this in markdown)

## What was the chapter about??

In chapter 13 we learned how we can save & data that we produced using different functions. This is helpful as many functions have a long computing time and we don't need rerun them all the time. Furthermore we learned how to hide them in our markdown document to make it easier to read and also less blown up.

## Chapter tasks

No tasks in chapter 13.

# chapter 14 Historic temperature scenarios

## What was the chapter about??

The chapter was about using temperature scenarios for generating synthetic weather data. The weather data we generated previously was based on observed temperature data for a short time period and only represents these climatic conditions. Nevertheless, climatic conditions are in general most often not constant and due to the man made climate change are going to change even faster in the future. We learned how to produce historic temperature scenarios for specific years from long term observed data that represent the normal climatic conditions of the period and can be used as input for the weather generator. These scenarios could be absolut scenarios, where they represented the variation around the mean temperatures of the specific year or relative scenarios, representing the relative change in temperature to a baseline scenario (which represents the climatic conditions of the period best). 

## Chapter tasks

### 1. For the location you chose for previous exercises, produce historic temperature scenarios representing several years of the historic record (your choice).

To produce historic temperature scenarios representing several years I will use the temperature_scenario_from_records() tool. The tool produces scenarios with monthly mean values for temperature minimum and maximum that are representative for particular years. As input it needs weather data as well as a reference year.
The scenarios I create will be relative temperature scenarios. Relative scenarios represent the change between a specific year and a year that best represents the temperature data, called baseline. This represents what should have been normal at that time and allows to better identify historic trends.
As input data I will use temperature records for the period 1973 to 2019. The year that represents the climate between 1973 and 2019 the best should be 1996, as it is the median year. Therefore I use this year as a baseline for my relative scenarios.
First I download and prepare the weather data for 1973 to 2019 using the code produced previously.
```{r, warning=FALSE, message=FALSE, eval=FALSE}

# download climate data for station Sevilla.
station_list<-handle_gsod(action="list_stations",
                          location=c(-6.2333,37.8667),
                          time_interval=c(1973,2019))

dehesa_weather <-handle_gsod(action="download_weather",
                     location=station_list$chillR_code[1],
                     time_interval=c(1973,2019))%>%
  handle_gsod()

# download climate data to fill the gaps.
patch_weather_dehesa <-
      handle_gsod(action = "download_weather",
                  location = as.character(station_list$chillR_code[c(5)]),
                  time_interval = c(1973,2019)) %>%
  handle_gsod()

# fill gaps for SEVILLA weather station with weather data from MORON AB.

dehesa_patches <- patch_daily_temperatures(
  weather = dehesa_weather$`SEVILLA`,
  patch_weather = patch_weather_dehesa$`MORON AB` 
)

# interpolate last missing dates.

Dehesa <-fix_weather(dehesa_patches)

Dehesa_temps <- Dehesa$weather
```
```{r,echo=FALSE,eval=FALSE}
write.csv(Dehesa_temps, "Dehesa_temps.csv", row.names = FALSE)
```
```{r,echo= FALSE, warning=FALSE, message=FALSE}
Dehesa_temps <- read.csv("D:/DatenVehlken/R/Projects/Chill_R/Dehesa_temps.csv")

```

From the downloaded data I first create a baseline scenario using the temperature_scenario_from_records() function. Furthermore I use the same tool to create temperature scenarios for our years of interest: 1989, 1991, 1996, 2001 and 2008. 
```{r, warning=FALSE, message=FALSE}
# creating baseline scenario.

scenario_1996 <- temperature_scenario_from_records(weather = Dehesa_temps,
                                                   year = 1996)
# creating scenarios for several years.

all_past_scenarios <- temperature_scenario_from_records(
  weather = Dehesa_temps,
  year = c(1989,
           1991,
           1996,
           2001,
           2008))

# adjusting the scenarios with the baseline scenario. Now the scenarios don't contain the absolute Tmin and Tmax mean values, but the relative change to the baseline scenario.

adjusted_scenarios <- temperature_scenario_baseline_adjustment(
  baseline = scenario_1996,
  temperature_scenario = all_past_scenarios)

# use these relative scenarios to generate temperature scenarios with means for Tmin and Tmax that correspond to the relative scenario. 
all_past_scenario_temps <- temperature_generation(
  weather = Dehesa_temps,
  years = c(1973,2019),
  sim_years = c(2001,2100),
  temperature_scenario = adjusted_scenarios)
```
```{r, echo=FALSE, warning=FALSE, message=FALSE, eval=FALSE}
save_temperature_scenarios(all_past_scenario_temps, "data", "Dehesa_hist_scenarios")
```

Now I want to calculate the chill accumulation for the scenarios. Instead of using the functions I created or used before I use the tempResponse_daily_list()-tool that is provided by the chillR package, which combines all the functions. Therefore I first define a model list. As I'm maybe interested in other agroclimatic metrics later I also implement models for frost and growing degree hours.
```{r, warning=FALSE, message=FALSE}

frost_model <- function(x)
  step_model(x,
             data.frame(
               lower=c(-1000,0),
               upper=c(0,1000),
               weight=c(1,0)))

models <- list(Chill_Portions = Dynamic_Model,
               GDH = GDH,
               Frost_H = frost_model)
```

Using the created list of historical scenarios as input the function computes temperature-based metrics defined by the input models.
```{r, warning=FALSE, message=FALSE}
chill_hist_scenario_list <- tempResponse_daily_list(all_past_scenario_temps,
                                                    latitude = 37.8667,
                                                    Start_JDay = 305,
                                                    End_JDay = 59,
                                                    models = models)
```

Before plotting I remove all incomplete winters.
```{r, warning=FALSE, message=FALSE}
chill_hist_scenario_list <- lapply(chill_hist_scenario_list,
                                   function(x) x %>%
                                     filter(Perc_complete == 100))
```
```{r, echo=FALSE, eval=FALSE}
save_temperature_scenarios(chill_hist_scenario_list, "data","Dehesa_hist_chill_305_59")
```

I now can plot the results using ggplot. 
```{r, warning=FALSE, message=FALSE}

# prepare the data for better usability in ggplot and filter out incomplete records.
scenarios <- names(chill_hist_scenario_list)[1:4]

all_scenarios <- chill_hist_scenario_list[[scenarios[1]]] %>%
  mutate(scenario = as.numeric(scenarios[1]))

for (sc in scenarios[2:4])
 all_scenarios <- all_scenarios %>%
  rbind(chill_hist_scenario_list[[sc]] %>%
          cbind(
            scenario=as.numeric(sc))
        ) %>%
  filter(Perc_complete == 100)


# computing the actual 'observed' chill for comparison.
actual_chill <- tempResponse_daily_list(Dehesa_temps,
                                        latitude=37.8667,
                                        Start_JDay = 305,
                                        End_JDay = 59,
                                        models)[[1]] %>%
  filter(Perc_complete == 100)


ggplot(data = all_scenarios,
       aes(scenario,
           Chill_Portions,
           fill = factor(scenario))) +
  geom_violin() +
  ylab("Chill accumulation (Chill Portions)") +
  xlab("Scenario year") +
  theme_bw(base_size = 15) +
  ylim(c(0,90)) +
  geom_point(data = actual_chill,
             aes(End_year,
                 Chill_Portions,
                 fill = "blue"),
             col = "blue",
             show.legend = FALSE) +
  scale_fill_discrete(name = "Scenario",
                      breaks = unique(all_scenarios$scenario)) 
```

Figure 13: Chill accumulation (in Chill Portions) for the relativ climate scenarios of the years 89 (red), 91 (yellow) 1996 (green) and 2001 (blue), as well as the actual observed chill (blue dots).

```{r, echo=FALSE, warning=FALSE, message=FALSE,eval=FALSE}
write.csv(actual_chill,"data/Dehesa_observed_chill_305_59.csv", row.names = FALSE)
```

# Chapter 15 Future temperature scenarios

## What was the chapter about??

The chapter was about the impacts of the future climate change due to man-made climate change and how we can implement them in our chill modelling by using Coupled Model Intercimparison Project (CMIP) climate models like RCPs or SSPs. These models are are recommended by the IPCC and get updated with every major report. 

## Chapter tasks

### 1.Briefly describe the differences between the RCPs and the SSPs.

Representative Concentration Pathways (RCPs) & Shared Socioeconomic Pathways(SSPs) are warming scenarios. 
The classical RCP scenarios are based on future greenhouse gas concentrations that seemed to be possible when the scenarios were set up. They are labeled after the change in radiative forcing values (W/m²) that is expected between 1750 and 2100.                                                                                                               The SSPs are instead based on qualitative descriptions of socio economic pathways that the mankind can follow (Sustainability, Middle of the Road, Regional Rivalry, Inequality, Fossil-fueled Development). To each of the SSPs an estimated change in C° is assigned. 

# Chapter 16 Making CMIP6 scenarios

## What was the chapter about??

The chapter was about how we can download the future temperature scenarios (CMIP6) discussed in chapter 15 from the Copernicus climate data store, generate change scenarios based on them and how to extract data that we can then use for chill modelling in chillR.

## Chapter tasks

### 1. Analyze the historic and future impact of climate change on two agroclimatic metrics of your choice, for the location you’ve chosen for your earlier analyses.

The agroclimatic metrics I analyse for the Dehesa San Francisco are chill portions and growing degree hours. To account for the historic impact I will use the historic data I already prepared. For the future impact I will use future climate scenarios. These climate scenarios can be downloaded using chillR. In current research SSPs are used, therefore we are also using them now. They can be downloaded from "COPERNICUS Database". The API provides the possibility to download them directly in chillR, using the download_cmip6_ecmwfr()-tool. I provide the tool information about the scenarios I want to download, as well as the area of interest and models to be used. The models are basically different modelling approaches to model the future climate, using different algorithms and taking into account different aspects. In addition I have to specify the time interval and frequency. Furthermore I have to specify the variables I want to download.
```{r, warning=FALSE, message=FALSE}

# preparing model input.

location=c(-6.2333,37.8667)

areas <- c(lat_high = 38, lon_low = -7.0, lat_low = 37, lon_high = -5.0)
```
```{r, eval=FALSE, warning=FALSE, message=FALSE}
# download scenarios.

download_cmip6_ecmwfr(
  scenarios = c("ssp126", "ssp245", "ssp370", "ssp585"),
  area = areas,
  user = '3ca1a864-ccfe-4be4-b52d-bae18478db98',
  key = '16136f09-1be6-48f0-8273-9de996df874e',
  model = 'default',
  frequency = 'monthly',
  variable = c('Tmin', 'Tmax'),
  year_start = 2015,
  year_end = 2100)
```

Like for the historical records I also need a baseline scenarios to which the scenarios refer. It is recommended that future- and baseline data are created with the same tool. Therefore I use the download_baseline_cmip6_ecmwfr()-tool. I download the baseline data for the years 1986-2014 as it is available in the Copernicus database and is with 29 years almost a complete climate normal interval.
```{r, eval=FALSE, warning=FALSE, message=FALSE}

# download baseline data.

download_baseline_cmip6_ecmwfr(
  area = areas,
  user = '3ca1a864-ccfe-4be4-b52d-bae18478db98',
  key = '16136f09-1be6-48f0-8273-9de996df874e',
  model = 'match_downloaded',
  frequency = 'monthly',
  variable = c('Tmin', 'Tmax'),
  year_start = 1986,
  year_end = 2014, 
  month = 1:12)
```

The downloaded scenario data is in raster format and therefore contains redundant information for me, as I'm only interested in one point. Therefore I extract the data for my point of interest, using the extract_cmip6_data() function.´
```{r, warning=FALSE, message=FALSE, eval=FALSE}

station <- data.frame(
  station_name = c("Sevilla"),
  longitude = c(-6.2333),
  latitude = c(37.8667))

extracted <- chillR::extract_cmip6_data(stations = station)
```

With the gen_rel_change_scenario() tool I generate change scenarios for the downloaded projections. To be usable for chillR I have to also transform them into a list, using the convert_scen_information() function.
```{r, warning=FALSE, message=FALSE, eval=FALSE}
change_scenarios <- gen_rel_change_scenario(extracted)

scen_list <- convert_scen_information(change_scenarios)
```

The baseline scenario I am using so far is based the Copernicus data set. Nevertheless, I still have the observed weather data from my location that I want to use. To take it into account I can use it to adjust the baseline of the newly downloaded data. Therefore I create a climate scenarios for the year 2000, which is also the baseline year in the generated data, and adjust it with the 1996 baseline scenario. 
```{r, warning=FALSE, message=FALSE, echo=FALSE}
Dehesa_temps<-read_tab("Dehesa_temps.csv")
```
```{r, eval=FALSE, warning=FALSE, message=FALSE}


temps_1996 <- temperature_scenario_from_records(Dehesa_temps,
                                                1996)

temps_2000 <- temperature_scenario_from_records(Dehesa_temps,
                                                2000)
base <- temperature_scenario_baseline_adjustment(temps_1996,
                                                 temps_2000)

scen_list <- convert_scen_information(change_scenarios, 
                                      give_structure = FALSE)

# now I can adjust the generated projections with the adjusted year 2000 scenario of the observed weather data. 

adjusted_list <- 
  temperature_scenario_baseline_adjustment(
    base,
    scen_list,
    temperature_check_args =
      list(scenario_check_thresholds = c(-5, 15)))

# the adjusted list I can now use for temperature generation.
temps <- temperature_generation(Dehesa_temps, 
                       years = c(1973, 2019),
                       sim_years = c(2001, 2100),
                       adjusted_list,
                       temperature_check_args =
                         list(scenario_check_thresholds = c(-5, 15)))
```
```{r,echo=FALSE,eval=FALSE}
save_temperature_scenarios(temps,
                            "future_climate",
                            "Dehesa_futuretemps")
```

The generated data I can now use again to plot the changes in temperature variables, using the already known workflows.
```{r message=FALSE, warning=FALSE}

frost_model <- function(x)
  step_model(x,
             data.frame(
               lower = c(-1000, 0),
               upper = c(0, 1000),
               weight = c(1, 0)))

models <- list(Chill_Portions = Dynamic_Model,
               GDH = GDH,
               Frost_H = frost_model)
```
```{r, eval=FALSE, warning=FALSE, message=FALSE}
chill_future_scenario_list <- tempResponse_daily_list(temps,
                                                    latitude = 37.8667,
                                                    Start_JDay = 305,
                                                    End_JDay = 59,
                                                    models = models)

chill_future_scenario_list <- lapply(chill_future_scenario_list,
                                     function(x) x %>%
                                       filter(Perc_complete == 100))
```
```{r,echo=FALSE,eval=FALSE}
save_temperature_scenarios(chill_future_scenario_list,
                           "future_climate",
                           "Dehesa_futurechill_305_59")
```
```{r,echo=FALSE}
chill_hist_scenario_list<-load_temperature_scenarios("data",
                                                     "Dehesa_hist_chill_305_59")
chill_future_scenario_list <- load_temperature_scenarios("future_climate",
                           "Dehesa_futurechill_305_59")
observed_chill <- read_tab("data/Dehesa_observed_chill_305_59.csv")
```

```{r, warning=FALSE, message=FALSE}

chills <- make_climate_scenario(
  chill_hist_scenario_list,
  caption = "Historical",
  historic_data = observed_chill,
  time_series = TRUE)

hist_chill_plots <- plot_climate_scenarios(
  climate_scenario_list = chills,
  metric = "Chill_Portions",
  metric_label = "Chill (Chill Portions)")
```

Figure 14: Chill (in Chill Portions) according to the Dynamic Model at the Dehesa San Francisco based on historic temperature scenarios (box plots) vs. actual chill (red dots) based on observed weather data.

To plot the data for all future climate scenarios I add each to the chill object. In a first step I identify the data for the SSP and Time combinations.
```{r, warning=FALSE, message=FALSE}
SSPs <- c("ssp126", "ssp245", "ssp370", "ssp585")
Times <- c(2050, 2085)

list_ssp <- 
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(2) %>%
  unlist()

list_gcm <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(3) %>%
  unlist()

list_time <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(4) %>%
  unlist()

for(SSP in SSPs)
  for(Time in Times)
    {
    
    # find all scenarios for the ssp and time.
    chill <- chill_future_scenario_list[list_ssp == SSP & list_time == Time]
    names(chill) <- list_gcm[list_ssp == SSP & list_time == Time]
    if(SSP == "ssp126") SSPcaption <- "SSP1"
    if(SSP == "ssp245") SSPcaption <- "SSP2"
    if(SSP == "ssp370") SSPcaption <- "SSP3"
    if(SSP == "ssp585") SSPcaption <- "SSP5"    
    if(Time == "2050") Time_caption <- "2050"
    if(Time == "2085") Time_caption <- "2085"
    chills <- chill %>% 
      make_climate_scenario(
        caption = c(SSPcaption,
                    Time_caption),
        add_to = chills)
}
```

Plotting all scenarios:
```{r, warning=FALSE, message=FALSE}
info_chill <-
  plot_climate_scenarios(
    climate_scenario_list = chills,
    metric = "Chill_Portions",
    metric_label = "Chill (Chill Portions)",
    texcex = 1.5)
```

Figure 15: Chill in Chill Portions for the Dehesa San Francisco according to the Dynamic Model for different climate models for all five SSP scenarios (right plots) for the year 2050 and 2085 & historical chill (left plots) based on historical temperature scenarios and observed chill.

In all years for all scenarios and models we can see lower expected chill as for the historical observations. As expected, we see the lowest reduction in chill for the SSP1 models, showing relatively similar distribution for 2050 and 2085. For the SSP2, SSP3 and SSP5 models in 2050 we see relatively low differences in the chill. For 2085 we can see a rapid decline in chill, indicating that some tipping points may be reached.
Next we look at the GDH:
```{r, warning=FALSE, message=FALSE}
info_heat <-
  plot_climate_scenarios(
    climate_scenario_list = chills,
    metric = "GDH",
    metric_label = "Heat (Growing Degree Hours)",
    texcex = 1.5)
```

Figure 16: Growing Degreee Hours at the Dehesa San Francisco for different climate models for all five SSP scenarios for the year 2050 and 2085 & historical chill.

In the plot for the growing hours, an opposite trend can be seen. Compared to the historic records, for all scenarios and models an increase in growing degree hours can be seen. Again, the SSP1 models show the lowest change with relatively similar hours for 2050 and 2085. For the SSP2, SSP3 and SSP5 models for 2050 again relatively similar GDH can be seen, with a slightly upwards trends from SSP2 to SSP5. For the year 2085 an high increase in GDH can be seen among the models.

# Chapter 17 Making CMIP5 scenarios with the ClimateWizard

## What was the chapter about??

The chapter was about how to download CMIP5 scenarios using the ClimateWizard instead of the Copernicus database and produce the same analysis like in chapter 16 with the data.

## Chapter tasks

No tasks

# Chapter 18 Plotting future scenarios

## What was the chapter about??

In the chapter we learned how to produce better plots for our future scenarios using the ggplot2 package and how to combine them using the patchwork package.

## Chapter tasks

### 1. Produce similar plots for the weather station you selected for earlier exercises.

As you may have noticed the analysed plots produced with the inbuilt chillR functions are not very clear. To produce clearer plots we can use ggplot, which has already a build in function in chillR.
For the plots I will use the scenario data that I produced before.
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(ggpmisc)
library(patchwork)
chill_hist_scenario_list <- load_temperature_scenarios("data",
                                                       "Dehesa_hist_chill_305_59")
actual_chill <- load_temperature_scenarios("data","Dehesa_observed_chill_305_59")

chill_future_scenario_list <- load_temperature_scenarios("future_climate","Dehesa_futurechill_305_59")
```
```{r, warning=FALSE, message=FALSE}

# using the make_climate_scenario() function to create climate scenarios for plotting. The function uses climate metric data that is stored in lists. I use the list I just generated.

chills <- make_climate_scenario(
  chill_hist_scenario_list,
  caption = "Historic",
  historic_data = actual_chill,
  time_series = TRUE)

# filtering the data for the scenarios.
SSPs <- c("ssp126", "ssp245", "ssp370", "ssp585")
Times <- c(2050, 2085)

list_ssp <- 
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(2) %>%
  unlist()

list_gcm <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(3) %>%
  unlist()

list_time <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(4) %>%
  unlist()


for(SSP in SSPs)
  for(Time in Times)
    {
    
    # find all scenarios for the ssp and time.
    chill <- chill_future_scenario_list[list_ssp == SSP & list_time == Time]
    names(chill) <- list_gcm[list_ssp == SSP & list_time == Time]
    if(SSP == "ssp126") SSPcaption <- "SSP1"
    if(SSP == "ssp245") SSPcaption <- "SSP2"
    if(SSP == "ssp370") SSPcaption <- "SSP3"
    if(SSP == "ssp585") SSPcaption <- "SSP5"    
    if(Time == "2050") Time_caption <- "2050"
    if(Time == "2085") Time_caption <- "2085"
    chills <- chill %>% 
      make_climate_scenario(
        caption = c(SSPcaption,
                    Time_caption),
        add_to = chills)
}
```

For ggplot using the data.frame format is recommended. Therefore I loop through our chill projection list and extract the data. First I do it for the historic data.
```{r, warning=FALSE, message=FALSE}

for(nam in names(chills[[1]]$data))
  {
   # Extract the data frame.
   ch <- chills[[1]]$data[[nam]]
   # Add columns for the new information we have to add and fill them.
   ch[,"GCM"] <- "none"
   ch[,"SSP"] <- "none"
   ch[,"Year"] <- as.numeric(nam)
   
   # Now check if this is the first time we've gone through this loop.
   # If this is the first time, the ch data.frame becomes the output
   # object (past_simulated).
   # If it is not the first time ('else'), we add the current data.frame
   # to the 'past_simulated' object
  if(nam == names(chills[[1]]$data)[1])
    past_simulated <- ch else
      past_simulated <- rbind(past_simulated,
                              ch)
  }

# add another column called 'Scenario' and label all rows as 'Historical' .
past_simulated["Scenario"] <- "Historical"
```

To add the historic observations to my data I create a pointer.
```{r, warning=FALSE, message=FALSE}
past_observed <- chills[[1]][["historic_data"]]
```

In the next step I loop through the future data and add it to the list.
```{r, warning=FALSE, message=FALSE}
# extract future data.
for(i in 2:length(chills))
  for(nam in names(chills[[i]]$data))
    {ch <- chills[[i]]$data[[nam]]
     ch[,"GCM"] <- nam
     ch[,"SSP"] <- chills[[i]]$caption[1]
     ch[,"Year"] <- chills[[i]]$caption[2]
     if(i == 2 & nam == names(chills[[i]]$data)[1])
       future_data <- ch else
         future_data <- rbind(future_data,ch)
  }
```

Now I can plot the data. Therefore I combine multiple plots using the plot_layout()-function, that is contained in the patchwork package. To prevent axis scales due to variation in the data I set the scales using the range()-function.
```{r, warning=FALSE, message=FALSE}
metric <- "GDH"
axis_label <- "Heat (in GDH)"

# get extreme values for the axis scale.

rng <- range(past_observed[[metric]],
             past_simulated[[metric]],
             future_data[[metric]]) 
```

Now I first produce a plot for the historic data. Therefore I generate box plots of the simulated historic data and combine them with a scatter plot of the observed historic data.
```{r, warning=FALSE, message=FALSE}
past_plot <- ggplot() +
  geom_boxplot(data = past_simulated,
               aes_string("as.numeric(Year)", metric, group = "Year"),
               fill = "skyblue",
               width = 0.9) +  # Increased boxplot width
  scale_y_continuous(limits = c(0, round(rng[2] + rng[2]/10))) +
  labs(x = "Year", y = axis_label) +
  facet_grid(~ Scenario) +
  theme_bw(base_size = 15) +
  geom_point(data = past_observed,
             aes_string("End_year", metric),
             col = "blue") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

past_plot
```

Figure 17: Heat (in GDH) at the Dehesa San Francisco based on historic temperature scenarios (sky blue box plots) vs. actual GDHs (blue dots) based on observed weather data.

Next I produce the future scenarios for the years 2050 and 2085 and combine them.
```{r, warning=FALSE, message=FALSE}
future_plot_list <- list()

time_points <- c(2050, 2085)

for(y in time_points)
{
  future_plot_list[[which(y == time_points)]] <-
    ggplot(data = future_data[which(future_data$Year==y),]) +
    geom_boxplot(aes_string("GCM",
                            metric,
                            fill="GCM")) +
    facet_wrap(vars(SSP), nrow = 1) +
    scale_x_discrete(labels = NULL,
                     expand = expansion(add = 1)) +
    scale_y_continuous(limits = c(0, 
                                  round(round(1.1*rng[2])))) +
    geom_text_npc(aes(npcx = "center",
                      npcy = "top", 
                      label = Year),
                  size = 5) +
    theme_bw(base_size = 15) +
    theme(axis.ticks.y = element_blank(),
          axis.text = element_blank(),
          axis.title = element_blank(),
          legend.position = "bottom",
          legend.margin = margin(0, 
                                 0, 
                                 0, 
                                 0, 
                                 "cm"),
          legend.background = element_rect(),
          strip.background = element_blank(),
          strip.text = element_text(face = "bold"),
          legend.box.spacing = unit(0, "cm"),
          plot.subtitle = element_text(
            hjust = 0.5,
            vjust = -1,
            size = 15 * 1.05,
            face = "bold")) 
}

# combining the plots.

both_plots <- past_plot + future_plot_list

plot <- both_plots +
           plot_layout(guides = "collect",
                       widths = c(1,rep(2,length(future_plot_list))))

plot <- plot & theme(legend.position = "bottom",
                     legend.text = element_text(size=8),
                     legend.title = element_text(size=10),
                     axis.title.x = element_blank())

plot
```

Figure 18: Heat in Growing Degree Hours for the Dehesa San Francisco according to the GDH model for different climate models (colors) for all five SSP scenarios (right plots) for the years 2050 and 2085 & historical heat (left plots) based on historical temperature (sky blue box plots) scenarios and observed heat (red dots).

I also produce plots for Chill Portions and Frost duration. For faster plotting of these and in the future I wrap up the code produced previously into a function.
```{r, warning=FALSE, message=FALSE}

# creating the function.

plot_scenarios_gg <- function(past_observed,
                              past_simulated,
                              future_data,
                              metric,
                              axis_label,
                              time_points)
{
  rng <- range(past_observed[[metric]],
               past_simulated[[metric]],
               future_data[[metric]])  
  past_plot <- ggplot() +
    geom_boxplot(data = past_simulated,
                 aes_string("as.numeric(Year)",
                            metric,
                            group="Year"),
                 fill="skyblue") +
    scale_y_continuous(limits = c(0, 
                                  round(round(1.1*rng[2])))) +
    labs(x = "Year", y = axis_label) +
    facet_grid(~ Scenario) +
    theme_bw(base_size = 15) +  
    theme(strip.background = element_blank(),
          strip.text = element_text(face = "bold"),
          axis.text.x = element_text(angle=45, 
                                     hjust=1)) +
    geom_point(data = past_observed,
               aes_string("End_year",
                          metric),
               col="blue")
  
  future_plot_list <- list()
  
  for(y in time_points)
  {
    future_plot_list[[which(y == time_points)]] <-
      ggplot(data = future_data[which(future_data$Year==y),]) +
      geom_boxplot(aes_string("GCM", 
                              metric, 
                              fill="GCM")) +
      facet_wrap(vars(SSP), nrow = 1) +
      scale_x_discrete(labels = NULL,
                       expand = expansion(add = 1)) +
      scale_y_continuous(limits = c(0, 
                                    round(round(1.1*rng[2])))) +
      geom_text_npc(aes(npcx = "center",
                        npcy = "top",
                        label = Year),
                    size = 5) +
      theme_bw(base_size = 15) +
      theme(axis.ticks.y = element_blank(),
            axis.text = element_blank(),
            axis.title = element_blank(),
            legend.position = "bottom",
            legend.margin = margin(0,
                                   0, 
                                   0, 
                                   0, 
                                   "cm"),
            legend.background = element_rect(),
            strip.background = element_blank(),
            strip.text = element_text(face = "bold"),
            legend.box.spacing = unit(0, "cm"),
            plot.subtitle = element_text(hjust = 0.5,
                                         vjust = -1,
                                         size = 15 * 1.05,
                                         face = "bold")) 
  }
  
  plot <- (past_plot +
             future_plot_list +
             plot_layout(guides = "collect",
                         widths = c(1,rep(2,length(future_plot_list))))
           ) & theme(legend.position = "bottom",
                     legend.text = element_text(size = 8),
                     legend.title = element_text(size = 10),
                     axis.title.x=element_blank())
  plot
  
}
```

Now I use the function to plot the scenarios.
```{r, warning=FALSE, message=FALSE}
plot_scenarios_gg(past_observed = past_observed,
                  past_simulated = past_simulated,
                  future_data = future_data,
                  metric = "Chill_Portions",
                  axis_label = "Chill (in Chill Portions)",
                  time_points = c(2050, 2085))
```

Figure 19: Chill in Chill Portions for the Dehesa San Francisco according to the Dynamic Model for different climate models (colors) for all five SSP scenarios (right plots) for the years 2050 and 2085 & historical chill (left plots) based on historical temperature (sky blue box plots) scenarios  and observed chill (red dots).

```{r, warning=FALSE, message=FALSE}
plot_scenarios_gg(past_observed = past_observed,
                  past_simulated = past_simulated,
                  future_data = future_data,
                  metric = "Frost_H",
                  axis_label = "Frost duration (in hours)",
                  time_points = c(2050, 2085))
```

Figure 20: Frost duration (in hours) for the Dehesa San Francisco according to the frost model for different climate models (colors) for all five SSP scenarios (right plots) for the years 2050 and 2085 & historical frost (left plots) based on historical temperature scenarios (sky blue box plots) and observed chill (red dots).

# Chapter 19 Chill model comparison

# chill model comparison at the Dehesa San Francisco
## What was the chapter about?

In the chapter chill models were compared. There are various chill models that can be used for phenology analysis. To choose the right one it might be useful to compare those models. A useful statistic for that is Safe Winter Chill. Safe Winter Chill is the 10% quantile of chill distribution, corresponding to chill amount that is exceeded in 90% of all years. We can compute the Safe Winter Chill for various chill model and produce heat maps for comparison. 

## Chapter tasks

### 1. Perform a similar analysis for the location you’ve chosen for your exercises.

As an example we will do this for the Dehesa San Francisco
```{r,warning=FALSE,message=FALSE,echo=FALSE}
library(chillR)
library(devtools)
library(dormancyR)
library(matlab)
library(tidyverse)
library(ggplot2)
```
```{r,warning=FALSE,message=FALSE,echo=FALSE}
library(colorRamps)
library(ggplot2)
library(patchwork)
```
First I create lists of the models I use. The list hourly_models contains models that are based on hourly temperature data, the daily_models list contains models based on daily data and store them together in a data.frame.
```{r,eval=FALSE}
# creating the model lists.

hourly_models <- list(Chilling_units = chilling_units,
     Low_chill = low_chill_model,
     Modified_Utah = modified_utah_model,
     North_Carolina = north_carolina_model,
     Positive_Utah = positive_utah_model,
     Chilling_Hours = Chilling_Hours,
     Utah_Chill_Units = Utah_Model,
     Chill_Portions = Dynamic_Model)

daily_models <- list(Rate_of_Chill = rate_of_chill,
                     Chill_Days = chill_days,
                     Exponential_Chill = exponential_chill,
                     # Triangular_Chill_Haninnen = triangular_chill_1,
                     Triangular_Chill_Legave = triangular_chill_2)

# storing the models together in a data.frame.

metrics <- c(names(daily_models),
             names(hourly_models))

model_labels = c("Rate of Chill",
                 "Chill Days",
                 "Exponential Chill",
                 # "Triangular Chill (Häninnen)",
                 "Triangular Chill (Legave)",
                 "Chilling Units",
                 "Low-Chill Chill Units",
                 "Modified Utah Chill Units",
                 "North Carolina Chill Units",
                 "Positive Utah Chill Units",
                 "Chilling Hours",
                 "Utah Chill Units",
                 "Chill Portions")
```
```{r,echo=FALSE}
# creating the model lists.

hourly_models <- list(Chilling_units = chilling_units,
     Low_chill = low_chill_model,
     Modified_Utah = modified_utah_model,
     North_Carolina = north_carolina_model,
     Positive_Utah = positive_utah_model,
     Chilling_Hours = Chilling_Hours,
     Utah_Chill_Units = Utah_Model,
     Chill_Portions = Dynamic_Model)

daily_models <- list(Rate_of_Chill = rate_of_chill,
                     Chill_Days = chill_days,
                     Exponential_Chill = exponential_chill,
                     # Triangular_Chill_Haninnen = triangular_chill_1,
                     Triangular_Chill_Legave = triangular_chill_2)

# storing the models together in a data.frame.

metrics <- c(names(daily_models),
             names(hourly_models))

model_labels = c("Rate of Chill",
                 "Chill Days",
                 "Exponential Chill",
                 # "Triangular Chill (Häninnen)",
                 "Triangular Chill (Legave)",
                 "Chilling Units",
                 "Low-Chill Chill Units",
                 "Modified Utah Chill Units",
                 "North Carolina Chill Units",
                 "Positive Utah Chill Units",
                 "Chilling Hours",
                 "Utah Chill Units",
                 "Chill Portions")
```
```{r,echo=FALSE,eval=FALSE}
data.frame(Metric = model_labels, 'Function name' = metrics)
```
```{r,eval=FALSE}
data.frame(Metric = model_labels, 'Function name' = metrics)
```

For comparison I apply the models to the observed temperature data as well as to the generated historical and future scenarios.
```{r}

# therefore I first load our temperature data.

Dehesa_temps <- read_tab("Dehesa_temps.csv")

Temps <- load_temperature_scenarios("data",
                                    "Dehesa_hist_scenarios")

future_temps <- load_temperature_scenarios("future_climate","Dehesa_futuretemps")
```

Now I apply the models to the past temperature scenarios. For the models using hourly data we again use the tempResponse_daily_list() function. For the models using daily data we use the tempResponse_list_daily() function of the dormancyR package instead, which is the equivalent tool for daily data.
```{r,eval=FALSE}

Start_JDay <- 305
End_JDay <- 59

daily_models_past_scenarios <- 
  tempResponse_list_daily(Temps,
                          Start_JDay = Start_JDay,
                          End_JDay = End_JDay,
                          models=daily_models)

# when using the daily-function you have to delete incomplete data manually.

daily_models_past_scenarios <- lapply(
  daily_models_past_scenarios,
  function(x) x[which(x$Perc_complete>90),])

hourly_models_past_scenarios<-
  tempResponse_daily_list(Temps,
                          latitude = 37.8667,
                          Start_JDay = Start_JDay,
                          End_JDay = End_JDay,
                          models = hourly_models,
                          misstolerance = 10)

past_scenarios <- daily_models_past_scenarios

# after conducting the models to both list I merge them to a single data frame, using the lapply()tool.

past_scenarios <- lapply(
  names(past_scenarios),
  function(x)
    cbind(past_scenarios[[x]],
          hourly_models_past_scenarios[[x]][,names(hourly_models)]))

names(past_scenarios) <- names(daily_models_past_scenarios)

# the same workflow I repeat for the observed temperature data.

daily_models_observed <- 
  tempResponse_daily(Dehesa_temps,
                     Start_JDay = Start_JDay,
                     End_JDay = End_JDay,
                     models = daily_models)

daily_models_observed <-
  daily_models_observed[which(daily_models_observed$Perc_complete>90),]

hourly_models_observed <- 
  tempResponse_daily_list(Dehesa_temps,
                          latitude=37.8667,
                          Start_JDay = Start_JDay,
                          End_JDay = End_JDay,
                          models = hourly_models,
                          misstolerance = 10)

past_observed <- cbind(
  daily_models_observed,
  hourly_models_observed[[1]][,names(hourly_models)])

```
```{r,echo=FALSE, eval=FALSE}
# for later use I save the scenarios.

save_temperature_scenarios(past_scenarios,
                           "data/future_climate",
                           "Dehesa_multichill_305_59_historic")
write.csv(past_observed,
          "data/future_climate/Dehesa_multichill_305_59_observed.csv",
          row.names=FALSE)
```


Next I compute the metrics for the future models. Like before I identify the data for the SSPs and Times combinations.
```{r,eval=FALSE}

SSPs <- c("ssp126", "ssp245","ssp370", "ssp585")
Times <- c(2050, 2085)


list_ssp <- 
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(2) %>%
  unlist()

list_gcm <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(3) %>%
  unlist()

list_time <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(4) %>%
  unlist()



# now I loop through the different combinations and apply the workflow I used for the observed and past scenarios.

for(SSP in SSPs)
  for(Time in Times)
    {
    Temps <- future_temps[list_ssp == SSP & list_time == Time]
    names(Temps) <- list_gcm[list_ssp == SSP & list_time == Time]
    daily_models_future_scenarios <- tempResponse_list_daily(
      Temps,
      Start_JDay = Start_JDay,
      End_JDay = End_JDay,
      models = daily_models)
    daily_models_future_scenarios<-lapply(
      daily_models_future_scenarios,
      function(x) x[which(x$Perc_complete>90),])
    hourly_models_future_scenarios<-
      tempResponse_daily_list(
        Temps,
        latitude = 37.8667,
        Start_JDay = Start_JDay,
        End_JDay = End_JDay,
        models=hourly_models,
        misstolerance = 10)

    future_scenarios <- daily_models_future_scenarios
    
    future_scenarios <- lapply(
      names(future_scenarios),
      function(x)
        cbind(future_scenarios[[x]],
              hourly_models_future_scenarios[[x]][,names(hourly_models)]))
    names(future_scenarios)<-names(daily_models_future_scenarios)
    
    chill<-future_scenarios
    
    save_temperature_scenarios(
      chill,
      "data/future_climate",
      paste0("Dehesa_multichill_305_59_",Time,"_",SSP))
}
```

In the next step I load the created scenarios and use them to generate chill scenarios for plotting, using the make_climate_scenario()-function.
```{r,echo=FALSE}
chill_past_scenarios <- load_temperature_scenarios(
  "future_climate",
  "Dehesa_multichill_305_59_historic")

chill_observed <- read_tab("future_climate/Dehesa_multichill_305_59_observed.csv")


```
```{r}
SSPs <- c("ssp126", "ssp245","ssp370", "ssp585")
Times <- c(2050, 2085)

# first I make the climate scenarios for the past scenarios and add the observed chill as historic data.
chills <- make_climate_scenario(chill_past_scenarios,
                                caption = "Historical",
                                historic_data = chill_observed,
                                time_series = TRUE)

# now I again loop through the SSP and time combinations for the future data and add the results to the list with the past and observed results.

for(SSP in SSPs)
  for(Time in Times)
    {
    chill <- load_temperature_scenarios(
      "future_climate",
      paste0("Dehesa_multichill_305_59_",Time,"_",SSP))
    if(SSP == "ssp126") SSPcaption <- "SSP1"
    if(SSP == "ssp245") SSPcaption <- "SSP2"
    if(SSP == "ssp370") SSPcaption <- "SSP3"
    if(SSP == "ssp585") SSPcaption <- "SSP5"    
    if(Time == "2050") Time_caption <- "2050"
    if(Time == "2085") Time_caption <- "2085"
    chills <- make_climate_scenario(chill,
                                    caption = c(SSPcaption,
                                                Time_caption),
                                    add_to = chills)
}
```

As stated before I want to crate heat maps with Safe Winter Chill metric for the models. For that I will use ggplot.
```{r}
# for that I first reorganize the data and safe it into a new data frame, to make it usable for ggplot. To compute Safe Winter Chill for the models and scenarios we use the quantile() function and use the results as input (quantile(ch$data[[nam]][,met],0.1)).

for(i in 1:length(chills))
   {ch <- chills[[i]]
   if(ch$caption[1] == "Historical")
     {GCMs <- rep("none",length(names(ch$data)))
      SSPs <- rep("none",length(names(ch$data)))
      Years <- as.numeric(ch$labels)
      Scenario <- rep("Historical",
                      length(names(ch$data)))} else
                        {GCMs <- names(ch$data)
                        SSPs <- rep(ch$caption[1],
                                    length(names(ch$data)))
                        Years <- rep(as.numeric(ch$caption[2]),
                                     length(names(ch$data)))
                        Scenario <- rep("Future",
                                        length(names(ch$data)))}
   
   for(nam in names(ch$data))
     {for(met in metrics)
       {temp_res <-
         data.frame(Metric = met,
                    GCM = GCMs[which(nam == names(ch$data))],
                    SSP = SSPs[which(nam == names(ch$data))],
                    Year = Years[which(nam == names(ch$data))],
                    Result = quantile(ch$data[[nam]][,met],0.1), 
                    Scenario = Scenario[which(nam == names(ch$data))])
       if(i == 1 & nam == names(ch$data)[1] & met == metrics[1])
         results <- temp_res else
           results <- rbind(results,
                            temp_res)
         }
     }
   }

# In the end I normalize the results by expressing them as changes to the baseline year 1989, as the chill models use different units.
for(met in metrics)
  results[which(results$Metric == met),"SWC"] <-
    results[which(results$Metric == met),"Result"]/
      results[which(results$Metric == met & results$Year == 1989),
              "Result"]-1
```

The new created data.frame I can use now to plot the data, using ggplot. 
```{r}
rng = range(results$SWC)

# to plot the future results I plot all results where GCM is not equal to none.

p_future <- ggplot(results[which(!results$GCM == "none"),],
                   aes(GCM,
                       y = factor(Metric,
                                  levels = metrics),
                       fill = SWC)) +
  geom_tile()

# to better see the single model results I rework the design of the plot.
p_future <-
  p_future +
  facet_grid(SSP ~ Year) +
  theme_bw(base_size = 15) +
  theme(axis.text = element_text(size=6))+
  scale_fill_gradientn(colours = matlab.like(15),
                       labels = scales::percent,
                       limits = rng) +
  theme(axis.text.x = element_text(angle = 75, 
                                   hjust = 1,
                                   vjust = 1)) +
  labs(fill = "Change in\nSafe Winter Chill\nsince 1980") +
  scale_y_discrete(labels = model_labels) +
  ylab("Chill metric")

p_future
```

Figure 21: Heatmap of change in Winter chill at the Dehesa San Francisco since 1980 according to various chill models for the SSPs projections 1,2,3 and 5 for the years 2050 and 2085. Red colors symbolize no change, while blue colors show a decrease.

I also create plots for the past scenarios.
```{r}
p_past<-
  ggplot(results[which(results$GCM == "none"),],
         aes(Year,
             y = factor(Metric, 
                        levels=metrics),
             fill = SWC)) +
  geom_tile()

p_past<-
  p_past +
  theme_bw(base_size = 15) +
  theme(axis.text = element_text(size = 6))

p_past<-
  p_past +
  scale_fill_gradientn(colours = matlab.like(20),
                       labels = scales::percent,
                       limits = rng)

p_past<-
  p_past +
  scale_x_continuous(position = "top") 

p_past<-
  p_past +
  labs(fill = "Change in\nSafe Winter Chill\nsince 1980") +
  scale_y_discrete(labels = model_labels) +
  ylab("Chill metric")

p_past
```

Figure 22: Heatmap of change in Safe Winter Chill at the Dehesa San Francisco since 1980 according to various chill models for the historic climate scenarios.

Now I combine them again using the patchwork package.
```{r}
chill_comp_plot<-
  (p_past +
     p_future +
     plot_layout(guides = "collect",
                 nrow = 2,
                 heights = c(1,3))) &
  theme(legend.position = "right",
        strip.background = element_blank(),
        strip.text = element_text(face = "bold"))

chill_comp_plot
```

Figure 23: Change in Safe Winter Chill at the Dehesa San Francisco since the year 1980, according to the different chill models. For the historic data, all models showed almost no change in SWC. For the future scenarios we can see mixed results. While most models showed an decrease between 0% and 150%, some model expect a decrease of up to 400%.

### 3. Produce an animated line plot of your results (summarizing Safe Winter Chill across all the GCMs).

It seems also be interesting to know how the SWC developed for the different SSPs through out the years. For that I will produce an animated line plot.
```{r}

# first I duplicate the historic results and assign them to to the different SSP scenarios to easier plot the time series.
hist_results <- results[which(results$GCM == "none"),]
hist_results$SSP <- "SSP1"
hist_results_2 <- hist_results
hist_results_2$SSP <- "SSP2"
hist_results_3 <- hist_results
hist_results_3$SSP <- "SSP3"
hist_results_4 <- hist_results
hist_results_4$SSP <- "SSP5"
hist_results <- rbind(hist_results,
                      hist_results_2,
                      hist_results_3,
                      hist_results_4)

# now I summarize the results.

future_results <- results[which(!results$GCM == "none"),]

GCM_aggregate <- aggregate(
  future_results$SWC,
  by=list(future_results$Metric,
          future_results$SSP,
          future_results$Year),
  FUN=mean)

colnames(GCM_aggregate) <- c("Metric",
                             "SSP",
                             "Year",
                             "SWC")

SSP_Time_series<-rbind(hist_results[,c("Metric",
                                       "SSP",
                                       "Year",
                                       "SWC")],
                       GCM_aggregate)
```

In the next step I create line plots showing the change for the SSP scenarios over time. 
```{r}
SSP_Time_series$Year <- as.numeric(SSP_Time_series$Year)

chill_change_plot<-
  ggplot(data = SSP_Time_series,
         aes(x = Year,
             y = SWC,
             col = factor(Metric,
                          levels = metrics))) +
  geom_line(lwd = 1.3) +
  facet_wrap(~SSP,
             nrow = 4) +
  theme_bw(base_size = 10) +
  labs(col = "Change in\nSafe Winter Chill\nsince 1980") +
  scale_color_discrete(labels = model_labels) +
  scale_y_continuous(labels = scales::percent) +
  theme(strip.background = element_blank(),
        strip.text = element_text(face = "bold")) +
  ylab("Safe Winter Chill")

chill_change_plot
```

Figure 24: Trend of change in Safe Winter Chill since 1980 at the Dehesa San Francisco for various chill models (colors) for the SSPs 1,2,3 and 5.

Using the animate()- function to create an animated line plot.
```{r,message=FALSE,warning=FALSE,echo=FALSE}
library(gganimate)
library(gifski)
library(png)
library(transformr)
```
```{r,message=FALSE,warning=FALSE}


ccp<-chill_change_plot +
  transition_reveal(Year)

animate(ccp, fps = 10)

```
```{r,echo=FALSE,eval=FALSE}
anim_save("data/chill_comparison_animation.gif",
          animation = last_animation())
```

Figure 25: Animated trend of change in Safe Winter Chill sincce 1980 at the Dehesa San Francisco for various chill models (colors) for the SSPs 1,2,3 and 5.

# Chapter 20 Simple phenology analysis

## What was the chapter about?

The chapter was about phenology modelling and gave an overview why it is tricky. In that context we learned about the issue of p-hacking and what is needed to avoid it.

## Chapter tasks

### 1. Provide a brief narrative describing what p-hacking is, and why this is a problematic approach to data analysis.

P-hacking is when researchers try to find the lowest p-values in their data sets, suggesting higher correlations, regardless if there really is a causal correlation between the variables. This leads to biased publications.

### 2. Provide a sketch of your causal understanding of the relationship between temperature and bloom dates.

Blooming is dependent on two temperature related processes: chilling and forcing. To survive winter, trees establish a dormant phase, called winter dormancy. Winter dormancy can be divided into the three phases: dormancy induction, endo dormancy and eco dormancy. During the induction phase the callose deposition in the plasmodesma leads to a stop in communication in the shoots, which causes the dormant state. To overcome this dormant state (endo dormancy) the trees have to be exposed to a certain amount of chilling as the temperatures lead to a degradation of the callose, unplugging the communication pathways and enables the meristems to grow. Now forcing temperatures are needed to lead to bloom, as the buds need exposure to heat to get into new phenological stages.

### 3. What do we need to know to build a process-based model from this?

The knowledge about what happens during dormancy. So after how much chilling the cummunication pathways are unplugged and how much forcing is needed for bloom. These requirements are specific to different cultivars.

# Chapter 21 Delineating temperature response phases with PLS regression.

## What was the chapter about?

We learned how to delineate chilling and forcing phases by analysing the results of a PLS regression. For that we learned how to conduct a PLS regression and how to plot the results using chillR functions.

## Chapter tasks

### 1. Briefly explain why you shouldn’t take the results of a PLS regression analysis between temperature and phenology at face value. What do you need in addition in order to make sense of such outputs?

PLS is a datamining technique that was developed to search for patterns in hugh data sets. The used dataset is small and even if the results show correlations, they don’t imply causality. To make sense of the output you need a bagged theory that supports the results.

### 2. Replicate the PLS analysis for the Roter Boskoop dataset that you used in a previous lesson.

The PLS analysis in chapter 21 determined the response of blooming dates of "Alexander Lucas" pears to temperature. The same I will do for 'Roter Boskoop' apples now. To delineate temperature response phases with PLS regression you can use the PLS_pheno()- tool in chillR. 
```{r, warning=FALSE, message=FALSE, eval=FALSE}

# first I convert the first bloom dates to Julian dates.

Roter_Boskop <- read_tab("data/Roter_Boskoop_bloom_1958_2019.csv") %>%
  select(Pheno_year, First_bloom) %>%
  mutate(Year = as.numeric(substr(First_bloom, 1, 4)),
         Month = as.numeric(substr(First_bloom, 5, 6)),
         Day = as.numeric(substr(First_bloom, 7, 8))) %>%
  make_JDay() %>%
  select(Pheno_year, JDay) %>%
  rename(Year = Pheno_year,
         pheno = JDay)
```

I also import the weather data and assign Julian dates.
```{r, warning=FALSE, message=FALSE, eval=FALSE}
CKA_temps <- read_tab("data/TMaxTMin1958-2019_patched.csv") %>%
  make_JDay
```

In the next step I conduct the PLS regression using the PLS_pheno()-function.
```{r, warning=FALSE, message=FALSE, eval=FALSE}
PLS_results <- PLS_pheno(CKA_temps,
                         Roter_Boskop)
```

The function produces a list containing two data frames. The results are contained in the PLS_summary data.frame, which we can now plot using the plot_PLS()-function. The function produces plots with VIP scores, model coefficients and daily accumulation rates of chill and heat metrics, here the Chill Portions as I use the Dynamic Model. Furthermore it stores the results in a table.
```{r, eval=FALSE, warning=FALSE, message=FALSE}
plot_PLS(PLS_results, "data/pls_roter_boskoop")
```
```{r,echo=FALSE, warning=FALSE, message=FALSE, out.width="75%"}
knitr::include_graphics("D:/DatenVehlken/R/Projects/Chill_R/data/pls_roter_boskoop.png")
```

Figure 26: Results of the PLS regression analysis of 'Roter Boskoop' at Campus Klein Altendorf. VIP values (upper plot), marking important days blue, the model coefficient (middle plot), marking important days red (when negative) or green (when positive), as well as the mean temperature (same sheme as model coefficients).

### 3. Write down your thoughts on why we’re not seeing the temperature response pattern we may have expected. What happened to the chill response? 

Maybe chilling does not always response the same to temperature. If it is to cold there may be no chilling and therefore the model could get confused. E.g. higher temperature lead to forcing, which is shown by the plots. But when it is to cold for chilling lower temperature don't lead to chilling. In that case warmer periods would lead to chilling again. Due to that we would have two responses to temperature for chilling, confusing the PLS regression.

# Chapter 22 Success and limitations of PLS regression analysis

## What was the chapter about?

In the chapter we learned about how PLS regression work and what their limits are. E.g. that they need variables with a monotonic relation ship.

## Chapter tasks

### 1. Briefly explain in what climatic settings we can expect PLS regression to detect the chilling phase - and in what settings this probably won’t work. 

We can expect PLS regression to detect the chilling phase in climatic settings with monotonic relationships, meaning lower temperatures are related to chilling and higher temperatures to forcing. When this monotonic relationship isn’t given in a region it won’t work. This is e.g. for regions where the temperatures are often lower than the effective chilling range specified by the used model. Then an temperature increase or higher temperatures would lead to an increase in chilling instead of reducing it. 

### 2. How could we overcome this problem?

To overcome this problem we could use other variables that have a monotonic relationship with chilling.

# Chapter 23 PLS regression with agroclimatic metrics

## What was the chapter about?

For PLS regression variables with a monotonic relationship to chill are needed. We learned how to use how to other agroclimatic metrics as input for PLS that may have a monotonic relation ship.

## Chapter tasks

### 1. Repeat the PLS_chill_force procedure for the ‘Roter Boskoop’ dataset. Include plots of daily chill and heat accumulation.

As stated temperature and chill effictiveness are not always monotonic related. To overcome the problem you can convert the temperature data to daily chill accumulation rates. This I will do for the Roter Boskoop at Klein Altendorf. 
```{r, warning=FALSE, message=FALSE, eval=FALSE}

# first I load our daily temperature data and convert it to hourly data.  

temps_hourly <- read_tab("data/TMaxTMin1958-2019_patched.csv") %>%
  stack_hourly_temps(latitude = 50.6)
```

Now I use the daily_chill()-function to calculate daily chill and heat accumulation. The function calculates these agroclimatic metrics based on hourly temperature data for a given set of chill models.
```{r, warning=FALSE, message=FALSE,eval=FALSE}

daychill <- daily_chill(hourtemps = temps_hourly,
                        running_mean = 11,
                        models = list(
                          Chilling_Hours = Chilling_Hours,
                          Utah_Chill_Units = Utah_Model,
                          Chill_Portions = Dynamic_Model,
                          GDH = GDH)
                        )
```

Next I can use the the PLS_chill_force() function to conduct a PLS regression analysis that relates the daily rates of chill and heat accumulation to biological phenomena that occure yearly. Here for the blooming dates of Roter Boskoop between 1958 and 2019. As Chill Portions accumulate step wise instead of continuously and these steps are not reached daily, I implement a running mean of 11 days  to avoid randomness in the estimation of the rates. 
```{r, warning=FALSE, message=FALSE,eval=FALSE}

# load and prepare data for the chill force function.

Boskop <- read_tab("data/Roter_Boskoop_bloom_1958_2019.csv") %>%
  select(Pheno_year, First_bloom) %>%
  mutate(Year = as.numeric(substr(First_bloom, 1, 4)),
         Month = as.numeric(substr(First_bloom, 5, 6)),
         Day = as.numeric(substr(First_bloom, 7, 8))) %>%
  make_JDay() %>%
  select(Pheno_year, 
         JDay) %>%
  rename(Year = Pheno_year,
         pheno = JDay)

# applying the chill force function.

plscf <- PLS_chill_force(daily_chill_obj = daychill,
                         bio_data_frame=Boskop,
                         split_month = 6,
                         chill_models = "Chill_Portions",
                         heat_models = "GDH",
                         runn_means = 11)
```

Next I plot the results with the plot_PLS()-function again and use the results to delineate chilling and forcing phases.
```{r, eval=FALSE, warning=FALSE, message=FALSE,eval=FALSE}

plot_PLS(plscf,
         PLS_results_path = "data/plscf_outputs_11days")
```
```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width="75%"}
knitr::include_graphics("D:/DatenVehlken/R/Projects/Chill_R/data/plscf_outputs_11days_Chill_Portions_GDH.png")
```

Figure 27: PLS regression results for 'Roter Boskoop' at CKA based on the Dynamic Model & GDH.

### 2. Run PLS_chill_force analyses for all three major chill models. Delineate your best estimates of chilling and forcing phases for all of them.

The three major chill models are the Chilling Hours model, the Utah model and the Dynamic Model.
I already calculated their chill metrics and can therefore just add them to my chill models list in the chill force function. As I already run the analysis for the Dynamic Model I skip it now. Neverteheless, the chilling phase I delineate from the Dynamic Model results for Roter Boskoop at CKA is from -26 to 67, while I would delineate the forcing phase from the 10 to 126.
```{r, warning=FALSE, message=FALSE,eval=FALSE}

plscf <- PLS_chill_force(daily_chill_obj = daychill,
                         bio_data_frame = Boskop,
                         split_month = 6,
                         chill_models = c("Chilling_Hours",
                                          "Utah_Chill_Units",
                                          "Chill_Portions"),
                       heat_models = c("GDH"))

```

To delineate the chilling and forcing phases for the other models I again use the plot_PLS() function.
The PLS_chill_force()-function created a list with tables for every chill and heat model combination. I now call the tables needed for plotting the different chill models and specify their axis label by the metric they compute. 
```{r, warning=FALSE, message=FALSE,eval=FALSE}
# plotting the Chilling Hours results.

plot_PLS(plscf[c(1,2,3)],
         PLS_results_path = "data/plfscf",
         axis_labels_chill_force = c("CH per day", "GDH per day"))


```
```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width="75%"}
knitr::include_graphics("D:/DatenVehlken/R/Projects/Chill_R/data/plfscf_Chilling_Hours_GDH.png")
```

Figure 28: PLS regression results for 'Roter Boskoop' based on Chilling Hours & GDH.

For the Chilling Hours model I delineate the chilling phase to the Julian days -22 to 65 , while the forcing phase is from days 8 to 123. 
```{r, warning=FALSE, message=FALSE,eval=FALSE}
# plotting the Utah model results.

plot_PLS(plscf[c(1,2,4)],
         PLS_results_path = "data/plfscf",
         axis_labels_chill_force = c("CU per day", "GDH per day"))


```
```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width="75%"}
knitr::include_graphics("D:/DatenVehlken/R/Projects/Chill_R/data/plfscf_Utah_Chill_Units_GDH.png")
```

Figure 29: PLS regression results for 'Roter Boskoop' at CKA based on the Utah-model & GDH.

For the Utah model I delineate the chilling phase to the Julian days -25 to 84 , while the forcing phase is from days 10 to 122.

### 3. Plot results for all three analyses, including shaded plot areas for the chilling and forcing periods you estimated.

Now I can plot the results with highlighted chilling and forcing phases.
```{r, warning=FALSE, message=FALSE,eval=FALSE}

# Dynamic Model

plot_PLS(plscf[c(1,2,5)],
         PLS_results_path = "data/plscf_high",
         add_chill = c(-26,67),
         add_heat = c(10,126))
```
```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width="75%"}
knitr::include_graphics("D:/DatenVehlken/R/Projects/Chill_R/data/plscf_high_Chill_Portions_GDH.png")
```

Figure 30: PLS regression results for 'Roter Boskoop' at CKA based on the Dynamic Model & GDH with highlighted chilling and forcing periods.

```{r, warning=FALSE, message=FALSE,eval=FALSE}
# Chilling hours

plot_PLS(plscf[c(1,2,3)],
         PLS_results_path = "data/plfscf_high",
         axis_labels_chill_force = c("CH per day", "GDH per day"),
         add_chill = c(-22,65),
         add_heat = c(8,123))
```
```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width="75%"}
knitr::include_graphics("D:/DatenVehlken/R/Projects/Chill_R/data/plfscf_high_Chilling_Hours_GDH.png")
```

Figure 31: PLS regression results for 'Roter Boskoop' based on Chilling Hours & GDH with highlighted chilling and forcing periods.

```{r, warning=FALSE, message=FALSE, eval=FALSE}

# Utah model

plot_PLS(plscf[c(1,2,4)],
         PLS_results_path = "data/plfscf_high",
         axis_labels_chill_force = c("CU per day", "GDH per day"),
         add_chill = c(-25,84),
         add_heat = c(10,122))


```
```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width="75%"}
knitr::include_graphics("D:/DatenVehlken/R/Projects/Chill_R/data/plfscf_high_Utah_Chill_Units_GDH.png")
```

Figure 31: PLS regression results for 'Roter Boskoop' at CKA based on the Utah-model & GDH with highlighted chilling and forcing periods.

# Chapter 24 Examples of PLS regression with agroclimatic  metrics

## What was the chapter about? 

The chapter contained examples of PLS regressions using agroclimatic metrics at different regions of the world. 

## Chapter tasks

### 1. Look across all the PLS results presented above. Can you detect a pattern in where chilling and forcing periods could be delineated clearly, and where this attempt failed?

Chilling and forcing periods could be clearly be determined in warmer regions with variable chill accumulation rates. In contrast, in the colder region without these variability it was hard to delineate clearly.

### 2. Think about possible reasons for the success or failure of PLS analysis based on agroclimatic metrics. Write down your thoughts.

As it was only possible to delineate chilling periods in region with variability in the chill accumulation, this is probably a factor for success or failure. The PLS looks for response to certain variables. If there is no variability in the data there might be just a low response that can be recognized by the PLS analysis.

# Chapter 25 Why PLS doesn't always work

## What was the chapter about?

The chapter was about chill model sensitivity. We learned how to analyse how different models response to temperature.

## Chapter tasks

```{r, warning=FALSE, message=FALSE,echo=FALSE}
library(ggplot2)
library(colorRamps)
```
### 1. Produce chill and heat model sensitivity plots for the location you focused on in previous exercises.

As suggested in the previous chapter variation in chill accumulation could influence the performance of PLS regression analysis. To check this I will create chill and heat model sensitivity plots that show the response of the (Dynamic) model to temperature at the Dehesa San Francisco. For the plots I need a data table with daily minimum and maximum temperatures and the corresponding chill portion value. To compute these I can use the functions created or used in previous chapters. To make the reproduction of this easier I will create a function called Chill_model_sensitivity(). It needs the latitude of the location, the models to be analyzed, the month of interest as well as a range for minimum and maximum temperatures as input.
```{r, warning=FALSE, message=FALSE, eval=FALSE}
Chill_model_sensitivity<-
  function(latitude,
           temp_models = list(Dynamic_Model = Dynamic_Model,
                              GDH = GDH),
           month_range = c(10, 11, 12, 1, 2, 3),
           Tmins = c(-10:20),
           Tmaxs = c(-5:30))
  {
    # create variables where the results of our analysis will be stored in.
  mins <- NA
  maxs <- NA
  metrics <- as.list(rep(NA,
                         length(temp_models)))
  names(metrics) <- names(temp_models)
  month <- NA
 # create a data table for our analysis. First I loop through all month specified and calculate the number of days for them.
  for(mon in month_range)
    {
    days_month <-
      as.numeric(difftime(ISOdate(2002,
                                  mon + 1,
                                  1),
                          ISOdate(2002,
                                  mon,
                                  1) ))
    if(mon == 12) days_month <- 31
    
    # in the next step I create all day weather tables for them.
    
    weather <- 
      make_all_day_table(data.frame(Year = c(2001, 2001),
                                    Month = c(mon, mon),
                                    Day = c(1, days_month),
                                    Tmin = c(0, 0),
                                    Tmax = c(0, 0)))

    # loop through all combinations of tmin and tmax where tmax >= tmin and create hourly temperature data.
    for(tmin in Tmins)
      for(tmax in Tmaxs)
        if(tmax >= tmin)
          {
          hourtemps <- weather %>%
            mutate(Tmin = tmin,
                   Tmax = tmax) %>%
            stack_hourly_temps(
              latitude = latitude) %>%
            pluck("hourtemps",
                  "Temp")
          # based on the hourly temperatures generated calculate the model metrics and normalize them by the number of days in that month.
          for(tm in 1:length(temp_models))
            metrics[[tm]] <- 
              c(metrics[[tm]],
                tail(do.call(temp_models[[tm]],
                        list(hourtemps)),1)/
                              days_month)
          
          mins <- c(mins, tmin)
          maxs <- c(maxs, tmax)
          month <- c(month, mon)
        }
  }
  
  # save the results in a data frame.
  results <- cbind(data.frame(Month = month,
                              Tmin = mins,
                              Tmax = maxs),
                   as.data.frame(metrics))
  
  results <- results[!is.na(results$Month),]
}
```

The created function I now apply to the latitude of the Dehesa San Francisco.
```{r, warning=FALSE, message=FALSE, eval=FALSE}
Model_sensitivities_Dehesa <-
  Chill_model_sensitivity(latitude = 37.881017,
                          temp_models = list(Dynamic_Model = Dynamic_Model,
                                             GDH = GDH),
                          month_range = c(10:12, 1:5))
```
```{r, warning=FALSE, message=FALSE, echo=FALSE, eval=FALSE}
write.csv(Model_sensitivities_Dehesa,
          "data/Model_sensitivities_Dehesa.csv",
          row.names = FALSE)
```

The data frame produced with this function creates a table with columns for Month, Tmin, Tmax and the model metrics.
As stated I want to produce plots showing the response of the (Dynamic) model to temperature at the Dehesa San Francisco.
These results I can use for plotting. For that I also generate a function and call it Chill_sensitivity_temps(). The function uses data produced by the sensitivity function, as well as observed temperature data as input. 
```{r, warning=FALSE, message=FALSE}
Chill_sensitivity_temps <-
  function(chill_model_sensitivity_table,
           temperatures,
           temp_model,
           month_range = c(10, 11, 12, 1, 2, 3),
           Tmins = c(-10:20),
           Tmaxs = c(-5:30),
           legend_label = "Chill/day (CP)")
{
# filter the sensitivity results for the month range we specified and create a new column with their names as factors.
  cmst <- chill_model_sensitivity_table
  cmst <- cmst[which(cmst$Month %in% month_range),]
  cmst$Month_names <- factor(cmst$Month,
                             levels = month_range,
                             labels = month.name[month_range])  
  
  # plot the Tmin vs Tmax values and use the value for the temperature model as fill. 
  DM_sensitivity<-
    ggplot(cmst,
           aes_string(x = "Tmin",
                      y = "Tmax",
                      fill = temp_model)) +
    geom_tile() +
    scale_fill_gradientn(colours = alpha(matlab.like(15),
                                         alpha = .5),
                         name = legend_label) +
    xlim(Tmins[1],
         Tmins[length(Tmins)]) +
    ylim(Tmaxs[1],
         Tmaxs[length(Tmaxs)])
  
  
  temperatures<-
    temperatures[which(temperatures$Month %in% month_range),]
  
  temperatures[which(temperatures$Tmax < temperatures$Tmin),
               c("Tmax", 
                 "Tmin")] <- NA
  
  temperatures$Month_names <-
    factor(temperatures$Month,
           levels = month_range,
           labels = month.name[month_range])  
  
  DM_sensitivity +
    geom_point(data = temperatures,
               aes(x = Tmin,
                   y = Tmax,
                   fill = NULL,
                   color = "Temperature"),
               size = 0.2) +
    facet_wrap(vars(Month_names)) +
    scale_color_manual(values = "black",
                       labels = "Daily temperature \nextremes (°C)",
                       name = "Observed at site" ) +
    guides(fill = guide_colorbar(order = 1),
           color = guide_legend(order = 2)) +
    ylab("Tmax (°C)") +
    xlab("Tmin (°C)") + 
    theme_bw(base_size = 15)

}
```

This function I now use to first plot the chill sensivity.
```{r,echo=FALSE}
Model_sensitivities_Dehesa <- read_tab("data/Model_sensitivities_Dehesa.csv")
```

```{r, warning=FALSE, message=FALSE}
Dehesa_weather <- read_tab("Dehesa_temps.csv")

Chill_sensitivity_temps(Model_sensitivities_Dehesa,
                        Dehesa_weather,
                        temp_model = "Dynamic_Model",
                        month_range = c(10, 11, 12, 1, 2, 3),
                        legend_label = "Chill per day \n(Chill Portions)") +
  ggtitle("Chill model sensitivity near Santa Olalla de Calla, Spain")
```

Figure 32: Sensitivity to temperature of the Dynamic Model at the Dehesa San Francisco.

Here you can see the chill per day as chill portions for october till march at the latitude of the Dehesa San Francisco. Blue colors indicate low chill portions while brown indicate higher chill portions (up to 1).

To plot the heat model sensitivity I just use the GDH model as input for my Chill_sensitivity_temps()-function.
```{r, warning=FALSE, message=FALSE}
Chill_sensitivity_temps(Model_sensitivities_Dehesa,
                        Dehesa_weather,
                        temp_model = "GDH",
                        month_range = c(12, 1:5),
                        legend_label = "Heat per day \n(GDH)") +
  ggtitle("Heat model sensitivity near Santa Olalla de Calla, Spain")
```

Figure 32: Sensitivity to temperature of the GDH model at the Dehesa San Francisco.

# Chapter 26 Evaluating PLS outputs

## What was the chapter about?

In chapter 26 the chilling and forcing requirements of the "Alexander Lucas" pear at Campus Klein Altendorf were analysed. As approxis for the agroclimatic needs of the trees the mean amounts of chill and heat that were accumulated during the chilling and forcing phases were used. For evaluating of the error the standard deviations were used. In addition the response of the cultivar to seasonal temperature was analysed by producing temperature response plots.

## Chapter tasks

### 1. Reproduce the analysis for the ‘Roter Boskoop’ dataset.

To calculate the chill and heat accumulated during the chilling and forcing phases for 'Roter Boskoop' apples at Campus Klein Altendorf I use the tempResponse()-function again. As input for the chill and heat phases I will use the days I delineated in chapter 23.
```{r, warning=FALSE, message=FALSE}
# load and transform the required data.
temps <- read_tab("data/TMaxTMin1958-2019_patched.csv")
temps_hourly <- temps %>%
  stack_hourly_temps(latitude = 50.6)

Boskop <- read_tab("data/Roter_Boskoop_bloom_1958_2019.csv") %>%
  select(Pheno_year, First_bloom) %>%
  mutate(Year = as.numeric(substr(First_bloom, 1, 4)),
         Month = as.numeric(substr(First_bloom, 5, 6)),
         Day = as.numeric(substr(First_bloom, 7, 8))) %>%
  make_JDay() %>%
  select(Pheno_year, 
         JDay) %>%
  rename(Year = Pheno_year,
         pheno = JDay)
```
```{r, warning=FALSE, message=FALSE}
# specify the delineated chill & heat phases.

chill_phase <- c(339, 67)
heat_phase <- c(10, 126)

# calculate chill accumulation.

chill <- tempResponse(hourtemps = temps_hourly,
                      Start_JDay = chill_phase[1],
                      End_JDay = chill_phase[2],
                      models = list(Chill_Portions = Dynamic_Model),
                      misstolerance = 10)

# calculate heat accumulation.

heat <- tempResponse(hourtemps = temps_hourly,
                     Start_JDay = heat_phase[1],
                     End_JDay = heat_phase[2],
                     models = list(GDH = GDH))
```

Now I can estimate the chill & heat requirements by calculate the mean accumulations and evaluate the error by computing the standard deviations.
```{r, warning=FALSE, message=FALSE}
chill_requirement <- mean(chill$Chill_Portions)
chill_req_error <- sd(chill$Chill_Portions)

heat_requirement <- mean(heat$GDH)
heat_req_error <- sd(heat$GDH)
```
```{r, eval=FALSE, warning=FALSE, message=FALSE}
chill_requirement
chill_req_error

heat_requirement
heat_req_error
```

The chillilling requirement is about 60.3 Chill Portions with an estimated error of 7.3 CP. The forcing requirement is ~6243 GDH, while the error is at ~1806 GDH.

To analyse the impacts of chilling and forcing temperatures on the phenology of Roter Boskop I create temperature response plots using the make_pheno_trend_plot()-function that is contained in chillR. The tool creates plots that show the phenology response  to temperatures during two phases plots.
```{r, warning=FALSE, message=FALSE,eval=FALSE}
chill_phase <- c(-26, 67)
heat_phase <- c(10, 126)


mpt <- make_pheno_trend_plot(weather_data_frame = temps,
                             pheno = Boskop,
                             Start_JDay_chill = chill_phase[1], 
                             End_JDay_chill = chill_phase[2],
                             Start_JDay_heat = heat_phase[1],
                             End_JDay_heat = heat_phase[2],
                             outpath = "data/",
                             file_name = "pheno_trend_plot",
                             plot_title =  "Impacts of chilling and forcing temperatures on Roter Boskoop phenology",
                             image_type = "png", 
                             colorscheme = "normal")

```
```{r,echo=FALSE, warning=FALSE, message=FALSE, out.width="50%"}
knitr::include_graphics("D:/DatenVehlken/R/Projects/Chill_R/data/pheno_trend_plot.png")
```

Figure 33: Temperature response plot of 'Roter Boskoop' at Campus Klein Altendorf. Red colors show late bloom dates while blue colors show early bloom dates.

Looking at the plot we can see a linear correlation between blooming dates and temperatures during chill and force periods. The latest bloom can be seen for low mean temperatures for chilling and forcing phase, while the earliest bloom can be seen for high mean temperatures for both. 

### 2. We’ve looked at data from a number of locations so far. How would you expect this surface plot to look like in Beijing? And how should it look in Tunisia?

For Beijing I would expect a relatively similar pattern to CKA, as the temperatures are similar. In Tunisia earlier bloom dates should occur with lower temperatures during the chilling phase, as the temperatures in Tunisia are in general relatively high. 

# Chapter 27 The relative importance of chill and heat

## What was the chapter about?

The chapter introduced us to the temperature response hypothesis.

## Chapter tasks

### 1. Describe the temperature response hypothesis outlined in this chapter.

The hypothesis is, that in cold regions where chill accumulation is always given the timing of the spring phase is mainly influenced by temperature variations during eco dormancy. In contrast, in high temperature areas where there are optimal conditions for forcing, the timing of the spring phase is driven by temperature variations in the endo dormancy phase. 

# Chapter 28 Experimentally enhanced PLS

## What was the chapter about?

We learned about how we can enhance our phenology data by using experimentally data. Therefore you create experiments where you expose trees to a number of different environments (climatic conditions), generating temperature response data that can be used to enhance the PLS analysis.

## Chapter tasks

No tasks.

# Chapter 29 Making valid tree phenology models

## What was the chapter about?

The chapter explained why and how to set up valid phenology models. We learned how we can validate our output as well as the process.

## Chapter tasks

### 1. Explain the difference between output validation and process validation.

In modelling output validation is refers to the procedure to check if a model is able to predict observations. For that different statistics like the Root Mean Square Error are used. Nevertheless, this gives no information about the reasonability of the model.Process validation is to check if the processes that are included in the model represent the biological processes it should model.

### 2. Explain what a validity domain is and why it is important to consider this whenever we want to use our model to forecast something.

The validity domain is the set of conditions in which a model remains meaningful. When the validity domain for a precipitation model is for elevations between 0m-500m, we shouldn't use it to model precipitation of a location at 1000m.

### 3. What is validation for purpose? 

The validation for purpose is to validate if the model is useful for the task we want to do. Again looking at the precipitation model example, when the location we want to use the model for is out of range of the validity domain (in the example higher than 1000m), the model is not uselful.

### 4. How can we ensure that our model is suitable for the predictions we want to make?

We should validate the performance of the model under conditions that represent the conditions that we want to make our predictions for.

# Chapter 30 The PhenoFlex model

## What was the chapter about?

The PhenoFlex model is a combination of the Dynamic Model and the GDH model and is contained in the chillR-package. It can transition between endo- and ecodormancy by translating accumulated chill(Dynamic Model) to heat effectiveness (GDH model) using a sigmoidal curve. The model allows to re calibrate the factors it uses and can therefore be fitted to certain cultivars or species. Together with knowledge about the dormancy processes of these cultivars it allows to develope process-based models.

## Chapter tasks

### 1. Parameterize the PhenoFlex model for `Roter Boskoop’ apples.

First I import the data for the Roter Boskoop blooming dates as well as hourly temperature data of the area and convert it in formats usable for chillR.
```{r, warning=FALSE, message=FALSE}
Boskop <-
  read_tab("data/Roter_Boskoop_bloom_1958_2019.csv") %>%
  select(Pheno_year, First_bloom) %>%
  mutate(Year = as.numeric(substr(First_bloom, 1, 4)),
         Month = as.numeric(substr(First_bloom, 5, 6)),
         Day = as.numeric(substr(First_bloom, 7, 8))) %>%
  make_JDay() %>%
  select(Pheno_year, JDay) %>%
  rename(Year = Pheno_year,
         pheno = JDay)

hourtemps <- 
  read_tab("data/TMaxTMin1958-2019_patched.csv") %>%
  stack_hourly_temps(latitude = 50.6)
```

The model can be used by the PhenoFlex() function. The function has 12 parameters that can be specified and relate to the dynamic model, the GDH model as well as the transition. As setting the parameters is difficult I first have to fit them. For that I use the solving process "Simulated Annealing". The process uses a set of initial parameters to predict outcomes and adjust the parameters based on the errors it encounters. As the knowledge about the results is limited, solvers are run multiple times which varying initial parameters. When fitting the parameters I use the phenologyFitter()-function instead of the PhenoFlex() function.
As initial parameters I use a provided set of parameters. Furthermore I specify upper and lower boundaries that allow the variation of the initial parameters of the solving process.  
```{r, warning=FALSE, message=FALSE}
# here's the order of the parameters (from the helpfile of the
# PhenoFlex_GDHwrapper function)
#          yc,  zc,  s1, Tu,    E0,      E1,     A0,         A1,   Tf, Tc, Tb,  slope
par <-   c(40, 190, 0.5, 25, 3372.8,  9900.3, 6319.5,
           5.939917e13,  4, 36,  4,  1.60)
upper <- c(41, 200, 1.0, 30, 4000.0, 10000.0, 7000.0,  
           6.e13, 10, 40, 10, 50.00)
lower <- c(38, 180, 0.1, 0 , 3000.0,  9000.0, 6000.0,   
           5.e13,  0,  0,  0,  0.05)
```

Now I generate a list of seasons containing temperature data and can be used in the phenologyFitter-function. For that we use the genSeasonList-function.
```{r, warning=FALSE, message=FALSE}
SeasonList <- genSeasonList(hourtemps$hourtemps,
                            mrange = c(8, 6),
                            years = c(1959:2018))
```

Next I use the phenologyFitter() to fit the parameters. I insert the initial parameters as well as the upper and lower bounderies and provide the temperature data. As model function I use the PhenoFlex_GDHwrapper, that automatically computes the growing degree hours and applies it to the PhenoFlex()-model. Furthermore I set the maximum number of iterations to 100 (maxit = 100). I stop the iteration when the model doesn't show improvement for 5 iterations. 
```{r, warning=FALSE, message=FALSE}

# fitting the parameters. 

Fit_res <- 
  phenologyFitter(par.guess = par, 
                  modelfn = PhenoFlex_GDHwrapper,
                  bloomJDays = Boskop$pheno[which(Boskop$Year > 1958)],
                  SeasonList = SeasonList,
                  lower = lower,
                           upper = upper,
                           control = list(smooth = FALSE,
                                          verbose = FALSE, 
                                          maxit = 100,
                                          nb.stop.improvement = 5))
# saving the model parameters.
Boskop_par <- Fit_res$par
```
```{r,eval=FALSE,echo=FALSE}
write.csv(Boskop_par,
          "data/PhenoFlex_parameters_Boskop.csv")
```

### 2. Produce plots of predicted vs. observed bloom dates and distribution of prediction errors.

Now I use the fitted parameters to predict the blooms for every year of our data & plot them. 
```{r, warning=FALSE, message=FALSE}
Boskop_par <- 
  read_tab("data/PhenoFlex_parameters_Boskop.csv")[,2]

SeasonList <- genSeasonList(hourtemps$hourtemps, 
                            mrange = c(8, 6),
                            years = c(1959:2019))

Boskop_PhenoFlex_predictions <- Boskop[which(Boskop$Year > 1958),]

for(y in 1:length(Boskop_PhenoFlex_predictions$Year))
   Boskop_PhenoFlex_predictions$predicted[y] <-
    PhenoFlex_GDHwrapper(SeasonList[[y]],
                         Boskop_par)

ggplot(Boskop_PhenoFlex_predictions,
       aes(x = pheno,
           y = predicted)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1) +
  theme_bw(base_size = 15) +
  xlab("Observed bloom date (Day of the year)") +
  ylab("Predicted bloom date (Day of the year)") +
  ggtitle("Predicted vs. observed bloom dates")
```

Figure 34: Predicted vs. observed bloom of "Roter Boskop" during 1958 and 2020 at CKA, according to the PhenoFlex model.

Furthermore I plot the distribution of the prediction errors.
```{r, warning=FALSE, message=FALSE}
# computing the errors.

Boskop_PhenoFlex_predictions$Error <- 
  Boskop_PhenoFlex_predictions$predicted - 
  Boskop_PhenoFlex_predictions$pheno

# plotting the distribution.

ggplot(Boskop_PhenoFlex_predictions,
       aes(Error)) +
  geom_histogram() +
  ggtitle("Distribution of prediction errors")
```

Figure 35: Distrubition of prediction errors of the PhenoFlex model of 'Roter Boskoop' at CKA.

### 3. Compute the model performance metrics RMSEP, mean error and mean absolute error.

```{r,eval=FALSE, warning=FALSE, message=FALSE}

RMSEP(Boskop_PhenoFlex_predictions$predicted,
      Boskop_PhenoFlex_predictions$pheno)

mean(Boskop_PhenoFlex_predictions$Error)

mean(abs(Boskop_PhenoFlex_predictions$Error))

```
 The RMSEP is at 4.69 days, while we can see a mean error of -1.92 days. The mean absolute error is at 3.34 days. The distribution of the prediction errors is as shown in the following histogram.

# Chapter 31 The PhenoFLex model - a second look

## What was the chapter about?

In the chapter the output of the PhenoFlex model was further analysed for the "Alexander Lucas" pears by conducting a temerature response analysis.

## Chapter tasks

### 1. Make chill and heat response plots for the ‘Roter Boskoop’ PhenoFlex model for the location you did the earlier analyses for.

To evaluate the temperature response of the PhenoFlex components by producing temperature response plots I can again use the workflow developed in chapter 25. As the PhenoFlex model works in a different way as the other models I have to adept the code a bit. Instead of calculating chilling portions with the dynamic model I compute chilling efficiency with the PhenoFlex model. In addition I calculate the heat efficiency, weight by the fitted parameters. 
First I set up a function to weigh the GDH by the parameters Tb, Tu and Tc of the PhenoFlex Model.
```{r, warning=FALSE, message=FALSE}


GDH_response <- function(T, par)
  {Tb <- par[11]
   Tu <- par[4]
   Tc <- par[10]
   GDH_weight <- rep(0, length(T))
   GDH_weight[which(T >= Tb & T <= Tu)] <-
     1/2 * (1 + cos(pi + pi * (T[which(T >= Tb & T <= Tu)] - Tb)/(Tu - Tb)))
   GDH_weight[which(T > Tu & T <= Tc)] <-
     (1 + cos(pi/2 + pi/2 * (T[which(T >  Tu & T <= Tc)] -Tu)/(Tc - Tu)))
  return(GDH_weight)
}
```

Now I make the described changes to the function of chapter 25 and run it.
```{r,eval=FALSE, warning=FALSE, message=FALSE}

latitude <- 50.6

month_range <- c(10, 11, 12, 1, 2, 3)

Tmins = c(-20:20)
Tmaxs = c(-15:30)

mins <- NA
maxs <- NA
chill_eff <- NA
heat_eff <- NA
month <- NA

simulation_par <- read_tab("data/PhenoFlex_parameters_Boskop.csv")[,2]

for(mon in month_range)
    {days_month <- as.numeric(difftime(ISOdate(2002, mon+1, 1),
                                       ISOdate(2002, mon, 1)))
     if(mon == 12) days_month <- 31
     weather <- 
       make_all_day_table(data.frame(Year = c(2002, 2002),                                   
                                     Month = c(mon, mon),
                                     Day = c(1, days_month),
                                     Tmin = c(0, 0),
                                     Tmax = c(0, 0)))
     
     for(tmin in Tmins)
      for(tmax in Tmaxs)
        if(tmax >= tmin)
          {
           hourtemps <- weather %>%
             mutate(Tmin = tmin,
                    Tmax = tmax) %>%
             stack_hourly_temps(latitude = latitude) %>%
             pluck("hourtemps", "Temp")
           
           chill_eff <- 
             c(chill_eff,
               PhenoFlex(temp = hourtemps,
                         times = c(1: length(hourtemps)),
                         A0 = simulation_par[7],
                         A1 = simulation_par[8],
                         E0 = simulation_par[5],
                         E1 = simulation_par[6],
                         Tf = simulation_par[9],
                         slope = simulation_par[12],
                         deg_celsius = TRUE,
                         basic_output = FALSE)$y[length(hourtemps)] /
                                            (length(hourtemps) / 24))
           
          heat_eff <- 
            c(heat_eff,
              cumsum(GDH_response(hourtemps,
                                  simulation_par))[length(hourtemps)] /
                                                 (length(hourtemps) / 24))
          mins <- c(mins, tmin)
          maxs <- c(maxs, tmax)
          month <- c(month, mon)
        }
}

results <- data.frame(Month = month,
                      Tmin = mins,
                      Tmax = maxs,
                      Chill_eff = chill_eff,
                      Heat_eff = heat_eff) %>%
  filter(!is.na(Month))
```
```{r,echo=FALSE,eval=FALSE}
write.csv(results,
          "data/model_sensitivity_PhenoFlex.csv")
```

To plot the chill & heat inefficiencies I use the Chill_sensitivity_temps()-function again, developed in chapter 25.
```{r, warning=FALSE, message=FALSE}

Model_sensitivities_PhenoFlex <-
  read.csv("data/model_sensitivity_PhenoFlex.csv")

CKA_weather <- read_tab("data/TMaxTMin1958-2019_patched.csv")

Chill_sensitivity_temps(Model_sensitivities_PhenoFlex,
                        CKA_weather,
                        temp_model = "Chill_eff",
                        month_range = c(10, 11, 12, 1, 2, 3),
                        Tmins = c(-20:20),
                        Tmaxs = c(-15:30),
                        legend_label = "Chill per day \n(arbitrary)") +
  ggtitle("PhenoFlex chill efficiency ('Roter Boskoop')")
```

Figure 36: PhenoFlex chill efficiency for 'Roter Boskoop' at CKA.

```{r, warning=FALSE, message=FALSE}
Chill_sensitivity_temps(Model_sensitivities_PhenoFlex,
                        CKA_weather,
                        temp_model = "Heat_eff",
                        month_range = c(10, 11, 12, 1, 2, 3),
                        Tmins = c(-20:20),
                        Tmaxs = c(-15:30),
                        legend_label = "Heat per day \n(arbitrary)") +
  ggtitle("PhenoFlex heat efficiency ('Roter Boskoop')")
```

Figure 37: PhenoFlex heat efficiency for 'Roter Boskoop' at CKA.

# Chapter 32 Can we improve the performance of PhenoFlex?

## What was the chapter about?

The chapter was about the PhenoFlex model and how to improve its performance by using experimental data.

## Chapter tasks

### 1. What was the objective of this work?

The main objective was to improve the PhenoFlex model performance by calibrating it with experimental phenology data.

### 2. What was the main conclusion?

The main conclusion is that the use of the PhenoFlex model is limitted for extreme conditions as there may be may be mechanisms for breaking dormancy that are not included in the model. 

### 3. What experiments could we conduct to test the hypothesis that emerged at the end of the conclusion?

To test the hypothesis we could conduct controlled experiments in which we grow trees in climate chambers and expose them to extremely high or extremely low temperatures. The data collected could be used to evaluate the model.

# Chapter 33 Frost risk analysis

## What was the chapter about?

Knowledge about frost risk is crucial for farmers to prevent frost damage. To choose the right cultivar for their location and to make investments in sutiable frost protection they need knowledge about the likelihood of frost events. We learned how to conduct a frost risk analysis by plotting phenology trends against frost occurrence.

## Chapter tasks

### 1. Download the phenology dataset for the apple cultivar Roter Boskoop from Klein-Altendorf.

First I load the data sets again.
```{r, warning=FALSE, message=FALSE}
Boskop <- read_tab("data/Roter_Boskoop_bloom_1958_2019.csv")

CKA_weather <- read_tab("data/TMaxTMin1958-2019_patched.csv")
```

As I want to plot the frost risks using ggplot I format the data.frame accordingly.
```{r, warning=FALSE, message=FALSE}

Boskop <- 
  Boskop %>%
  pivot_longer(cols = "First_bloom":"Last_bloom",
               names_to = "variable",
               values_to="YEARMODA") %>%
  mutate(Year = as.numeric(substr(YEARMODA, 1, 4)),
         Month = as.numeric(substr(YEARMODA, 5, 6)),
         Day = as.numeric(substr(YEARMODA, 7, 8))) %>%
  make_JDay() 
```

### 2. Illustrate the development of the bloom period over the duration of the weather record. Use multiple ways to show this - feel free to be creative

To illustrate the development of the blooming periods for 'Roter Boskoop' between 1958 and 2019 I first create a linear plot.
```{r, warning=FALSE, message=FALSE}
ggplot(data = Boskop,
       aes(Pheno_year,
           JDay,
           col = variable)) +
  geom_line() +
  theme_bw(base_size = 15) +
  scale_color_discrete(name = "Phenological event",
                       labels = c("First bloom",
                                  "Full bloom", 
                                  "Last bloom")) +
  xlab("Phenological year") +
  ylab("Julian date (day of the year)") +
  geom_smooth(method = "lm")
```

Figure 38: Line plot of Julian dates of first(red), full(green) and last(blue) bloom of 'Roter Boskoop' at CKA for the years 1958 to 2020. Bold lines represent the linear trend,the gray shades the standard deviation. The plot shows a trend towards earlier days of the year.

To better visualize the trends I smooth the linear plot.
```{r, warning=FALSE, message=FALSE}
ggplot(data=Boskop,aes(Pheno_year,JDay,col=variable)) +
  geom_smooth() +
  theme_bw(base_size=15) +
  scale_color_discrete(
    name = "Phenological event",
    labels = c("First bloom", "Full bloom", "Last bloom")) +
  xlab("Phenological year") +
  ylab("Julian date (day of the year)") 
```

Figure 39: Smoothed line plot of Julian dates of first(red), full(green) and last(blue) bloom of 'Roter Boskoop' at CKA for the years 1958 to 2020. Lines represent the linear trend,the gray shades the standard deviation. The smoothed plot shows the trend more clearly.

### 3. Evaluate the occurrence of frost events at Klein-Altendorf since 1958. Illustrate this in a plot.

To evaluate the occurrence of frost events at CKA since 1958 I use the frost model again, but adept it a little bit.As I'm not interested in the total frost hours but only if a frost event occurs I don't sum up the frost hours by setting sum to FALSE. Instead I compute the daily frost hours.
```{r, warning=FALSE, message=FALSE}

# setting up the frost model data frame.

frost_df = data.frame(
  lower = c(-1000, 0),
  upper = c(0, 1000),
  weight = c(1, 0))

# setting up the frost model withoung summing up frost hours.
frost_model_no_summ <- 
  function(x) step_model(x, 
                         frost_df,
                         summ=FALSE)

# create hourly temperatures.

hourly <- stack_hourly_temps(CKA_weather,
                             latitude = 50.625)
# compute the frost hours.

frost <- tempResponse(hourly,
                      models = c(frost = frost_model))

# classify hours in hourly temperature data as frost in a new column. 

hourly$hourtemps[, "frost"] <- frost_model_no_summ(hourly$hourtemps$Temp)


# sum up frost hours for every day.

Daily_frost_hours <- aggregate(hourly$hourtemps$frost,
                               by = list(hourly$hourtemps$YEARMODA),
                               FUN = sum)

# assign frost hours to Julian day temperature data.

Daily_frost <- make_JDay(CKA_weather)

Daily_frost[, "Frost_hours"] <- Daily_frost_hours$x
```

Now I can plot the daily frost hours for 1958 to 2019 using ggplot.
```{r, warning=FALSE, message=FALSE}
Daily_frost$Frost_hours[which(Daily_frost$Frost_hours == 0)] <- NA

ggplot(data = Daily_frost,
       aes(Year,
           JDay,
           size = Frost_hours)) +
  geom_point(col = "light blue",
             alpha = 0.8) + 
  scale_size(range = c(0, 3),
             breaks = c(1, 5, 10, 15, 20),
             labels = c("1", "5", "10", "15", "20"),
             name = "Frost hours") +
  theme_bw(base_size = 15)
```

Figure 40: Daily frost hours at Campus Klein Altendorf since 1958, represented by blue dots. Daily frost hours represented by dot scale.

### 4. Evaluate the occurrence of frost events at Klein-Altendorf since 1958. Illustrate this in a plot.

Therefore I enhance the previously produced plot. As I'm only interested in the spring period I filter out the other days by limiting the y-axis values by setting ylim(c(75, 140)). To be able to better see when frost occurs during bloom I colorize the plot by assigning different colors if the frost occurs before, during or after bloom. Therefore I first create a new data.frame from 'Roter Boskoop' with single columns for first full and last bloom of the year.
```{r, warning=FALSE, message=FALSE}

Ribbon_Boskop <-
  Boskop %>%
  select(Pheno_year, variable, JDay) %>%
  pivot_wider(names_from = "variable", values_from = "JDay")

# to identify frost events that overlap with bloom I add the bloom events to the daily frost data set and assign every day of the year the value if they are before the first bloom, during bloom or after the last bloom. 
lookup_dates <- Ribbon_Boskop

row.names(lookup_dates) <- lookup_dates$Pheno_year

Daily_frost[, "First_bloom"]<-
  lookup_dates[as.character(Daily_frost$Year),
               "First_bloom"]

Daily_frost[, "Last_bloom"]<-
  lookup_dates[as.character(Daily_frost$Year),
               "Last_bloom"]

Daily_frost[which(!is.na(Daily_frost$Frost_hours)),
            "Bloom_frost"] <-
  "Before bloom"

Daily_frost[which(Daily_frost$JDay >= Daily_frost$First_bloom),
            "Bloom_frost"]<-
  "During bloom"

Daily_frost[which(Daily_frost$JDay > Daily_frost$Last_bloom),
            "Bloom_frost"]<-
  "After bloom"

Daily_frost[which(Daily_frost$JDay > 180),
            "Bloom_frost"]<-
  "Before bloom"
```

Now I plot the frost events for the spring between 1958 and 2019 together with the blooming events and colorize them accordingly to the described demands.
```{r, warning=FALSE, message=FALSE}

ggplot(data = Ribbon_Boskop,
       aes(Pheno_year)) +
  geom_ribbon(aes(ymin = First_bloom, 
                  ymax = Last_bloom),
              fill = "light gray") +
  geom_line(aes(y = Full_bloom)) +
  theme_bw(base_size = 15) +
  xlab("Phenological year") +
  ylab("Julian date (day of the year)") +
  geom_point(data = Daily_frost,
             aes(Year,
                 JDay,
                 size = Frost_hours,
                 col = Bloom_frost),
             alpha = 0.8) + 
  scale_size(range = c(0, 5),
             breaks = c(1, 5, 10, 15, 20),
             labels = c("1", "5", "10", "15", "20"),
             name = "Frost hours") +
  scale_color_manual(
    breaks = c("Before bloom",
               "During bloom",
               "After bloom"),
    values = c("light green",
               "red",
               "light blue"),
    name = "Frost timing") +
  theme_bw(base_size = 15) +
  ylim(c(75, 140))
```

Figure 41: Relationship of daily frost at CKA since 1958 to bloom events. Frost hours are symbolized by dots. Green dots represent frost before bloom, red during bloom and sky blue after bloom.

### 5. Evaluate how the risk of spring frost for this cultivar has changed over time. Has there been a significant trend?

To evaluate how the risk of sprng frost for Roter Boskoop changed over the time I conduct a linear regression analysis and a Kendall test. 
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(stargazer)
```
```{r,eval=FALSE, warning=FALSE, message=FALSE}

# first for each year of our record I separately calculate the sum of frost hours that occurs before, during or after bloom.

Bloom_frost_trend <- 
  aggregate(
    Daily_frost$Frost_hours,
    by = list(Daily_frost$Year,
              Daily_frost$Bloom_frost),
    FUN = function(x) sum(x,
                          na.rm = TRUE))

colnames(Bloom_frost_trend) <- c("Year",
                                 "Frost_timing",
                                 "Frost_hours")
# filter out only the the frost hours that occurred during bloom
DuringBloom<-
  Bloom_frost_trend[
    which(Bloom_frost_trend$Frost_timing == "During bloom"),]

# conduct the linear regression and the Kendall test.

LM <- lm(DuringBloom$Frost_hours ~ DuringBloom$Year)
stargazer(LM, type ="text")

Kendall(x = DuringBloom$Year,
        y = DuringBloom$Frost_hours)
```

The linear regression shows an yearly increase in 0.05 hours frost during bloom for Roter Boskoop at CKA. Nevertheless, the results for the Kendall (tau = 0.0333 & p = 0.74) test don't show that there is a significant trend.

# Bibliography

Junta Andalucia, n.d. Dirección General de la Producción Agrícola y Ganadera [WWW Document]. URL http://www.juntadeandalucia.es/agriculturaypesca/fitEmaWeb/faces/pages/infoEstacion.xhtml?id=102 (accessed 2.26.25).

Kottek, M., Grieser, J., Beck, C., Rudolf, B., Rubel, F., 2006. World map of the Köppen-Geiger climate classification updated.
